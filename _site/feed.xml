<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Anyinlover Blog</title>
    <description>在这里探索数据，发现真理</description>
    <link>http://anyinlover.github.io/</link>
    <atom:link href="http://anyinlover.github.io/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Sun, 24 Apr 2016 23:10:32 +0800</pubDate>
    <lastBuildDate>Sun, 24 Apr 2016 23:10:32 +0800</lastBuildDate>
    <generator>Jekyll v2.4.0</generator>
    
      <item>
        <title>因子分析</title>
        <description>&lt;p&gt;在前面的混合高斯模型中，我们常常假定我们有充足的样本去发现数据的内在结构。也就是样本数m远远大于特征数n。&lt;/p&gt;

&lt;p&gt;现在考虑&lt;script type=&quot;math/tex&quot;&gt;n \gg n&lt;/script&gt;的情况，在这样的条件下，单高斯模型都无法拟合，更不论混合高斯模型了。根据最大似然估计，高斯模型的拟合参数如下：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
\mu &amp;= \frac {1} {m} \sum_{i=1}^m x^{(i)} \\
\Sigma &amp;= \frac {1} {m} \sum_{i=1}^m (x^{(i)}-\mu) (x^{(i)}-\mu)^T
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;我们会发现&lt;script type=&quot;math/tex&quot;&gt;\Sigma&lt;/script&gt;是奇异阵，&lt;script type=&quot;math/tex&quot;&gt;\Sigma^{-1}&lt;/script&gt;不存在。这样就无法计算模型的密度函数了。&lt;/p&gt;

&lt;p&gt;进一步讲，要通过最大似然估计来拟合高斯模型，必须让m远大于n，才能有比较好的结果。&lt;/p&gt;

&lt;p&gt;那么我们如何解决这个样本数不足的问题呢？&lt;/p&gt;

&lt;h2 id=&quot;sigma&quot;&gt;限制&lt;script type=&quot;math/tex&quot;&gt;\Sigma&lt;/script&gt;&lt;/h2&gt;
&lt;p&gt;如果我们没有足够的数据来拟合一个协方差矩阵，我们可以为其添加一些限制。比如考虑协方差矩阵是对角的，也就是特征间是相互独立的，在这种情况下：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Sigma_{jj} = \frac {1} {m} \sum_{i=1}^m (x_j^{(i)}-\mu_j)^2&lt;/script&gt;

&lt;p&gt;二元高斯分布在平面的投影是个椭圆，对角阵意味着椭圆轴线与坐标轴平行。&lt;/p&gt;

&lt;p&gt;进一步，我们还能控制&lt;script type=&quot;math/tex&quot;&gt;\Sigma&lt;/script&gt;不仅是对角的，而且对角元素相同。&lt;script type=&quot;math/tex&quot;&gt;\Sigma= \sigma^2 I&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;\sigma^2&lt;/script&gt;可以通过最大似然估计得到：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sigma^2 = \frac {1}{mn} \sum_{j=1}^n \sum_{i=1}^m (x_j^{(i)}-\mu_j)^2 &lt;/script&gt;

&lt;p&gt;在高斯分布平面投影上椭圆变成了圆。&lt;/p&gt;

&lt;p&gt;假如不对&lt;script type=&quot;math/tex&quot;&gt;\Sigma&lt;/script&gt;做限制，我们必须在&lt;script type=&quot;math/tex&quot;&gt;m \geq n+1&lt;/script&gt;的条件下才能保证&lt;script type=&quot;math/tex&quot;&gt;\Sigma&lt;/script&gt;不是一个奇异阵，在上面的约束下，只需要&lt;script type=&quot;math/tex&quot;&gt;m \geq 2&lt;/script&gt;就能保证非奇异。&lt;/p&gt;

&lt;p&gt;但上述的假设太强，意味着特征之间完全相互独立，假如我们想要挖掘数据内部的关系时，就需要使用到因子分析模型。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;边缘和条件高斯分布&lt;/h2&gt;
&lt;p&gt;在描述因子分析之前，我们先来讨论一下如何找到联合多元高斯分布的条件分布和边缘分布。&lt;/p&gt;

&lt;p&gt;假定我们有一个随机变量：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
x = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}
&lt;/script&gt;

&lt;p&gt;这里&lt;script type=&quot;math/tex&quot;&gt;x_1 \in \mathbb{R}^r, x_2 \in \mathbb{R}^s, \Sigma_{11} \in \mathbb{R}^{r\times r}, \Sigma_{12} \in \mathbb{R}^{r \times s}&lt;/script&gt; 假定 &lt;script type=&quot;math/tex&quot;&gt; x \sim \mathcal{N} (\mu, \Sigma) &lt;/script&gt;，有：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
 \mu = \begin{bmatrix} \mu_1 \\ \mu_2 \end{bmatrix}, 
\Sigma = \begin{bmatrix} \Sigma_{11} &amp; \Sigma_{12} \\ \Sigma_{21} &amp; \Sigma_{22} \end{bmatrix}  %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;x_1&lt;/script&gt; 和&lt;script type=&quot;math/tex&quot;&gt;x_2&lt;/script&gt;被称为联合多元分布，那么&lt;script type=&quot;math/tex&quot;&gt;x_1&lt;/script&gt;的边缘分布是什么？很容易可以看到&lt;script type=&quot;math/tex&quot;&gt;E[x_1]=\mu_1&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;Cov(x_1) = E[(x_1-\mu_1)(x_1-\mu_1)]=\Sigma_{11}&lt;/script&gt;
。因为根据协方差的定义：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
Cov(x) &amp;= \Sigma \\
&amp;= \begin{bmatrix} \Sigma_{11} &amp; \Sigma_{12} \\ \Sigma_{21} &amp; \Sigma_{22}
    \end{bmatrix} \\
&amp;= E[(x-\mu)(x-\mu)^T] \\
&amp;= E[\begin{pmatrix} x_1-\mu_1 \\ x_2-\mu_2 \end{pmatrix}
{\begin{pmatrix} x_1-\mu_1 \\ x_2-\mu_2 \end{pmatrix}}^T] \\
&amp;= E \begin{pmatrix} (x_1-\mu_1)(x_1-\mu_1)^T (x_1-\mu_1)(x_2-\mu_2)^T \\
(x_2 - \mu_2)(x_1 - \mu_1)^T (x_2-\mu_2)(x_2-\mu_2)^T \end{pmatrix}
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;因此可以得出&lt;script type=&quot;math/tex&quot;&gt;x_1&lt;/script&gt;的边缘分布是&lt;script type=&quot;math/tex&quot;&gt;x_1 \sim \mathcal{N} (\mu_1, \Sigma_{11})&lt;/script&gt;。&lt;/p&gt;

&lt;p&gt;下面再来考虑在&lt;script type=&quot;math/tex&quot;&gt;x_2&lt;/script&gt;给定下&lt;script type=&quot;math/tex&quot;&gt;x_1&lt;/script&gt;的条件分布。可以记作&lt;script type=&quot;math/tex&quot;&gt;x_1 \mid x_2 \sim \mathcal{N} (\mu_{1\mid 2}, \Sigma_{1 \mid 2})&lt;/script&gt;，可以计算如下：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
\mu_{1 \mid 2} &amp;= \mu_1 + \Sigma_{12} \Sigma_{22}^{-1} (x_2 - \mu_2) \\
\Sigma_{1 \mid 2} &amp;= \Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21}
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;h2 id=&quot;section-1&quot;&gt;因子分析模型&lt;/h2&gt;
&lt;p&gt;在因子分析模型中，我们定义(x,z)的联合分布如下，其中&lt;script type=&quot;math/tex&quot;&gt;z \in \mathbb{R}^k &lt;/script&gt;是一个潜在随机变量：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
z &amp; \sim \mathcal{N}(0, I) \\
x \mid z &amp; \sim \mathcal{N} (\mu + \Lambda z, \Psi)
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;其中向量&lt;script type=&quot;math/tex&quot;&gt;\mu \in \mathcal{R}^n &lt;/script&gt;，矩阵 &lt;script type=&quot;math/tex&quot;&gt; \Lambda \in \mathcal{R}^{n \times k} &lt;/script&gt;， 对角阵&lt;script type=&quot;math/tex&quot;&gt;\Psi \in \mathcal{R}^{n \times n}&lt;/script&gt;，k的取值一般都要小于n。&lt;/p&gt;

&lt;p&gt;我们相当于把数据从k维映射到n维&lt;script type=&quot;math/tex&quot;&gt;\mu+ \Lambda z&lt;/script&gt;，最后再填上一个噪音&lt;script type=&quot;math/tex&quot;&gt;\Psi&lt;/script&gt;。上面的因子分析模型也可以表示成下面的形式：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
z &amp; \sim \mathcal{N} (0, I) \\
\epsilon &amp; \sim \mathcal{N} (0, \Psi) \\
x &amp; = \mu + \Lambda z + \epsilon
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;我们的随机变量z，x构成了一个联合高斯分布：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\begin{bmatrix} z \\ x \end{bmatrix} \sim \mathcal{N} (\mu_{zx}, \Sigma)
&lt;/script&gt;

&lt;p&gt;下面来找到&lt;script type=&quot;math/tex&quot;&gt;\mu_{zx}&lt;/script&gt;和&lt;script type=&quot;math/tex&quot;&gt;\Sigma&lt;/script&gt;。&lt;/p&gt;

&lt;p&gt;容易直到&lt;script type=&quot;math/tex&quot;&gt;E[z]=0&lt;/script&gt;，因为z满足标准正态分布。&lt;script type=&quot;math/tex&quot;&gt;E[x]=\mu&lt;/script&gt;可有下式求解：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
E[x] &amp;= E[\mu+ \Psi z + \epsilon] \\
&amp;= \mu + \Psi E[z] + E[\epsilon] \\
&amp;= \mu
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;因此可以得到&lt;script type=&quot;math/tex&quot;&gt;\mu_{zx}&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mu_{zx} =
\begin{bmatrix}
\overrightarrow{0} \\
\mu
\end{bmatrix}
&lt;/script&gt;

&lt;p&gt;下面继续计算&lt;script type=&quot;math/tex&quot;&gt;\Sigma&lt;/script&gt;，因为&lt;script type=&quot;math/tex&quot;&gt;\Sigma_{zz} = Cov(z) = I&lt;/script&gt;，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
\Sigma_{zx} &amp;= E[(z - E[z])(x - E[x])^T] \\
&amp;= E[z (\mu + \Lambda z + \epsilon - \mu)^T ] \\
&amp;= E[zz^T] \Lambda^T + E [z \epsilon^T] \\
&amp;= \Lambda^T
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
\Sigma_{xx} &amp;= E[(x - E[x])(x - E[x])^T] \\
&amp;= E[(\mu + \Lambda z + \epsilon - \mu)(\mu + \Lambda z + \epsilon - \mu)^T ] \\
&amp;= E[ \Lambda z z^T \Lambda^T + \epsilon z^T \Lambda^T + \Lambda z \epsilon^T + \epsilon \epsilon^T] \\
&amp;= \Lambda E[z z^t] \Lambda^T + E [\epsilon \epsilon^T] \\
\Lambda \Lambda^T + \Psi
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;最终我们得到联合分布如下：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{bmatrix}
z \\ x
\end{bmatrix}
\sim \mathcal{N} \left (
\begin{bmatrix}
\overrightarrow{0} \\ \mu
\end{bmatrix},
\begin{bmatrix}
I &amp; \Lambda^T \\
\Lambda &amp; \Lambda \Lambda^T + \Psi
\end{bmatrix}
\right)
 %]]&gt;&lt;/script&gt;

&lt;p&gt;x的边缘分布是&lt;script type=&quot;math/tex&quot;&gt;x \sim \mathcal{N} (\mu, \Lambda \Lambda^T + \Psi) &lt;/script&gt;，因此可以得出其最大似然函数的表达式：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\ell (\mu, \Lambda, \Psi) = \log \prod_{i=1}^m \frac {1} {2\pi)^{n/2}
{| \Lambda\Lambda^T + \Psi |}^{1/2} } \exp \left( - \frac{1}{2}
(x^{(i)}-\mu)^T (\Lambda \Lambda^T + \Psi)^{-1} (x^{(i)} - \mu) \right) &lt;/script&gt;

&lt;p&gt;对于上述的最大似然函数估计，同样的无法直接求解，需要用最大期望算法来解决。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;因子分析的最大期望算法&lt;/h2&gt;

&lt;p&gt;在E步，我们需要计算&lt;script type=&quot;math/tex&quot;&gt;Q_i(z^{(i)}) = p(z^{(i)} \mid x^{(i)}; \mu, \Lambda, \Psi)。根据前面条件分布的公式，我们知道&lt;/script&gt; z^{(i)} \mid x^{(i)}; \mu, \Lambda, \Psi \sim \mathcal{N} (\mu&lt;em&gt;{z^{(i)} \mid x^{(i)}}, \Sigma&lt;/em&gt;{z^{(i)} \mid x^{(i)}})，其中：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
\mu_{z^{(i)} \mid x^{(i)}} &amp;= \Lambda^T (\Lambda\Lambda^T + \Psi)^{-1} (x^{(i)} - \mu) \\
\Sigma_{z^{(i)} \mid x^{(i)}} &amp;= I - \Lambda^T (\Lambda\Lambda^T + \Psi)^{-1} \Lambda
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;后面的推导偷懒不写了~一句话就是用EM算法去求解，感觉更多的是考验数学水平。&lt;/p&gt;

</description>
        <pubDate>Sun, 24 Apr 2016 00:00:00 +0800</pubDate>
        <link>http://anyinlover.github.io/2016/04/24/factor-analysis/</link>
        <guid isPermaLink="true">http://anyinlover.github.io/2016/04/24/factor-analysis/</guid>
        
        <category>Ng机器学习系列</category>
        
        <category>机器学习理论</category>
        
        
      </item>
    
      <item>
        <title>最大期望算法</title>
        <description>&lt;p&gt;在前一讲中，我们谈到最大期望算法应用于混合高斯模型中，在这一讲，我们给出最大期望算法的一般形式，展示其如何能运用于求解潜在变量的预测问题。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;琴生不等式&lt;/h2&gt;

&lt;p&gt;假如是一个凸函数，即&lt;script type=&quot;math/tex&quot;&gt;f&#39;&#39;(x) \geq 0&lt;/script&gt;，或者&lt;script type=&quot;math/tex&quot;&gt;H \geq 0&lt;/script&gt;。恒取大于号是称为严格凸函数。X是随机分布，琴生不等式可表达如下：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; E|f(X)| \geq f(EX)&lt;/script&gt;

&lt;p&gt;当等号成立时必须有&lt;script type=&quot;math/tex&quot;&gt;X=E \mid X \mid &lt;/script&gt;恒成立，即X是常量。&lt;/p&gt;

&lt;p&gt;琴生不等式可以用图形直观的去解释。&lt;/p&gt;

&lt;p&gt;同理，当f时一个凹函数时，不等式反向成立。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;最大期望算法&lt;/h2&gt;

&lt;p&gt;给定训练集&lt;script type=&quot;math/tex&quot;&gt;\{x^{(1)},\cdots,x^{(m)}\}&lt;/script&gt;由m个独立样本构成。我们需要针对数据拟合模型&lt;script type=&quot;math/tex&quot;&gt;p(x,z)&lt;/script&gt;的参数，最大似然函数由下式给出：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
\ell(\theta) &amp;= \sum_{i=1}^m \log p(x;\theta) \\
&amp;= \sum_{i=1}^m \log \sum_z p(x,z;\theta)
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;对上式直接求偏导计算无法得到解析解。因为&lt;script type=&quot;math/tex&quot;&gt;z^{(i)}&lt;/script&gt;是一个潜在变量，只有&lt;script type=&quot;math/tex&quot;&gt;z^{(i)}&lt;/script&gt;是已知的条件下，才可能解出，因此引出了最大期望算法。它的策略就是分两步走，E步首先给出&lt;script type=&quot;math/tex&quot;&gt;\ell&lt;/script&gt;的下限，然后在M步优化这个下限。&lt;/p&gt;

&lt;p&gt;对每个i，令&lt;script type=&quot;math/tex&quot;&gt;Q_i&lt;/script&gt;是对z的分布（&lt;script type=&quot;math/tex&quot;&gt;\sum_z Q_i(z) = 1, Q_i(z) \geq 0&lt;/script&gt;）运用琴生不等式，有下面的关系：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
\sum_i \log p(x^{(i)}; \theta) &amp;= \sum_i \log \sum_{z^{(i)}} p(x^{(i)}, z^{(i)}; \theta) \\
&amp;= \sum_i \log \sum_{z^{(i)}} Q_i(z^{(i)}) \frac {p(x^{(i)}, z^{(i)}; \theta)} {Q_i(z^{(i)})} \\
&amp;\geq \sum_i \sum_{z^{(i)}} Q_i(z^{(i)}) \log \frac {p(x^{(i)}, z^{(i)}; \theta)} {Q_i(z^{(i)})}
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;我们可以看出分布&lt;script type=&quot;math/tex&quot;&gt;[p(x^{(i)}，z^{(i)};\theta)/Q_i(z^{(i)})]&lt;/script&gt;针对&lt;script type=&quot;math/tex&quot;&gt;z^{(i)}&lt;/script&gt;关于&lt;script type=&quot;math/tex&quot;&gt;Q_i&lt;/script&gt;的期望值就是&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{z^{(i)}} Q_i(z^{(i)}) \frac {p(x^{(i)}, z^{(i)}; \theta)} {Q_i(z^{(i)})}&lt;/script&gt;

&lt;p&gt;又由于对数函数是一个凹函数，因此有：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f\left(E_{z^{(i)} \sim Q_i} \left[ \frac {p(x^{(i)}, z^{(i)}; \theta)} {Q_i(z^{(i)})} \right] \right) \geq E_{z^{(i)} \sim Q_i} \left[ f \left( \frac {p(x^{(i)}, z^{(i)}; \theta)} {Q_i(z^{(i)})} \right) \right] &lt;/script&gt;

&lt;p&gt;因此对于任意分布&lt;script type=&quot;math/tex&quot;&gt;Q_i&lt;/script&gt;，上式给出了对&lt;script type=&quot;math/tex&quot;&gt;\ell(\theta)&lt;/script&gt;的下限。在选择&lt;script type=&quot;math/tex&quot;&gt;Q_i&lt;/script&gt;时，一个很自然的做法是l令琴生不等式等号成立。即变量为常量：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac {p(x^{(i)}, z^{(i)}; \theta)} {Q_i(z^{(i)}    )} = c&lt;/script&gt;

&lt;p&gt;其中c是独立于&lt;script type=&quot;math/tex&quot;&gt;z^{(i)}&lt;/script&gt;的常量。又因为&lt;script type=&quot;math/tex&quot;&gt;\sum_z Q_i(z^{(i)})=1&lt;/script&gt;，所以有：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
Q_i(z^{(i)}) &amp;= \frac {p(x^{(i)}, z^{(i)}; \theta)} {\sum_z p(x^{(i)}, z^{(i)}; \theta)} \\
&amp;= \frac {p(x^{(i)}, z^{(i)}; \theta)} {p(x^{(i)}; \theta)} \\
&amp;= p(z^{(i)} \mid x^{(i)}; \theta)
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;也就是&lt;script type=&quot;math/tex&quot;&gt;Q_i&lt;/script&gt;是在&lt;script type=&quot;math/tex&quot;&gt;x^{(i)}&lt;/script&gt;给定下&lt;script type=&quot;math/tex&quot;&gt;z^{(i)}&lt;/script&gt;的后验分布。现在我们可以得出最大期望算法的数学表达：
重复下面的步骤直到收敛：{&lt;/p&gt;

&lt;p&gt;E步，对每个i，令：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q_i(z^{(i)}) := p(z^{(i)} \mid x^{(i)}; \theta) &lt;/script&gt;

&lt;p&gt;M步，令：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta := \arg \max_\theta \sum_i \sum_{z^{(i)}} Q_i(z^{(i)}) \log \frac {p(x^{(i)}, z^{(i)}; \theta)} {Q_i(z^{(i)})} &lt;/script&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;但我们如何能证明最大期望算法一定能收敛呢？通过下式可以得到：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
\ell(\theta^{(t+1)}) &amp; \geq \sum_i \sum_{z^{(i)}} Q_i^{(t)} (z^{(i)}) \log \frac {p(x^{(i)}, z^{(i)}; \theta^{(t+1)})} {Q^{(t)}_i(z^{(i)})} \\
&amp; \geq \sum_i \sum_{z^{(i)}} Q_i^{(t)} (z^{(i)}) \log \frac {p(x^{(i)}, z^{(    i)}; \theta^{(t)})} {Q^{(t)}_i(z^{(i)})} \\
&amp;= \ell(\theta^{(t)})
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;因此最大期望算法一定是逐渐收敛的，在实际应用中，我们常常是给定一个容忍系数来终止算法拟合。&lt;/p&gt;

&lt;p&gt;假如我们定义：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(Q, \theta) = 
\sum_i \sum_{z^{(i)}} Q_i(z^{(i)}) \log \frac {p(x^{(i)}, z^{(i)}; \theta)} {Q_i(z^{(i)})} &lt;/script&gt;

&lt;p&gt;最大期望算法可以被视作对于J的坐标上升法。在E步优化Q，在M步优化&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;混合高斯模型再回顾&lt;/h2&gt;

&lt;p&gt;有了最大期望算法的一般定义，我们再回头来拟合混合高斯模型的参数。&lt;/p&gt;

&lt;p&gt;E步很简单：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\omega_j^{(i)}=Q_i(z^{(i)}=j)=P(z^{(i)}=j \mid x^{(i)}; \phi, \mu, \Sigma)&lt;/script&gt;

&lt;p&gt;在M步，我们需要分别对参数&lt;script type=&quot;math/tex&quot;&gt;\phi, \mu, \Sigma&lt;/script&gt;求目标函数的最大化:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
&amp; \sum_{i=1}^m \sum_{z^{(i)}} Q_i(z^{(i)}) \log \frac {p(x^{(i)},z^{(i)}; \phi, \mu, \Sigma)}
{Q_i(z^{(i)})} \\
=&amp; \sum_{i=1}^m \sum_{j=1}^k Q_i(z^{(i)}=j) \log \frac {p(x^{(i)} \mid z^{(i)}=j; \mu, \Sigma)
p(z^{(i)}=j; \phi)} {Q_i(z^{(i)}=j)} \\
=&amp; \sum_{i=1}^m \sum_{j=1}^k \omega_j^{(i)} \log \frac {\frac{1} {(2\pi)^{n/2} |\Sigma_j|^{1/2}}
\exp (-\frac{1}{2} (x^{(i)}-\mu_j)^T \Sigma_j^{-1} (x^{(i)}-\mu_j)) \cdot \phi_j} { \omega_j^{(i)}} \\
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;首先来求解&lt;script type=&quot;math/tex&quot;&gt;\mu_j&lt;/script&gt;，对其求梯度可以得到：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
&amp; \nabla_{\mu_j} \sum_{i=1}^m \sum_{j=1}^k \omega_j^{(i)} \log \frac {\frac{1} {(2\pi)^{n/2} |\Sigma_j|^{1/2}}
121 \exp (-\frac{1}{2} (x^{(i)}-\mu_j)^T \Sigma_j^{-1} (x^{(i)}-\mu_j)) \cdot \phi_j} { \omega_j^{(i)    }} \\
=&amp; -\nabla_{\mu_j}\sum_{i=1}^m \sum_{j=1}^k \omega_j^{(i)} \frac{1}{2} (x^{(i)}-\mu_j)^T \Sigma_j^{-1} (x^{(i)}-\mu_j) \\
=&amp; \frac{1}{2} \sum_{i=1}^m \omega_j^{(i)} \nabla_{\mu_j} (2\mu_j^T\Sigma_j^{-1}x^{(i)} -\mu_j^T\Sigma_j^T \mu_j) \\
=&amp; \sum_{i=1}^m \omega_j^{(i)} (\Sigma_j^{-1}x^{(i)}-\Sigma_j^T \mu_j)
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;将上式等于0可以得到：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mu_j := \frac {\sum_{i=1}^m \omega_j^{(i)}x^{(i)}} {\sum_{i=1}^m \omega_j^{(i)}}&lt;/script&gt;

&lt;p&gt;再来考虑&lt;script type=&quot;math/tex&quot;&gt;\phi_j&lt;/script&gt;，我们需要最大化：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; \sum_{i=1}^m \sum_{j=1}^k \omega_j^{(i)} \log \phi_j &lt;/script&gt;

&lt;p&gt;但认识到&lt;script type=&quot;math/tex&quot;&gt;\phi_j&lt;/script&gt;并不是完全独立的，存在&lt;script type=&quot;math/tex&quot;&gt; \sum_{j=1}^k \phi_j = 1 &lt;/script&gt;的关系，因此我们可以构造拉格朗日方程：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; \mathcal{L} (\phi) = \sum_{i=1}^m \sum_{j=1}^k \omega_j^{(i)} \log \phi_j + \beta (\sum_{j=1}^k \phi_j - 1) &lt;/script&gt;

&lt;p&gt;求偏导我们得到：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; \frac {\partial} {\partial \phi_j} \mathcal{L} (\phi) = \sum_{i=1}^m \frac {\omega_j^{(i)}} {\phi_j} + \beta &lt;/script&gt;

&lt;p&gt;因此有：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; \phi_j = \frac { \sum_{i=1}^m \omega_j^{(i)} } { -\beta } &lt;/script&gt;

&lt;p&gt;根据&lt;script type=&quot;math/tex&quot;&gt; \sum_{j} \phi_j = 1 &lt;/script&gt;的约束关系，&lt;script type=&quot;math/tex&quot;&gt; -\beta = \sum_{i=1}^m \sum_{j=1}^k \omega_j^{(i)} = \sum_{i=1}^m 1 = m &lt;/script&gt;，最终我们可以得到：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\phi_j := \frac {1} {m} \sum_{i=1}^m \omega_j^{(i)} &lt;/script&gt;

&lt;p&gt;关于&lt;script type=&quot;math/tex&quot;&gt;\Sigma_j&lt;/script&gt; 也可以通过类似方法得到，不过让我自己推出来还是蛮困难的~&lt;/p&gt;
</description>
        <pubDate>Thu, 21 Apr 2016 00:00:00 +0800</pubDate>
        <link>http://anyinlover.github.io/2016/04/21/em-algorithm/</link>
        <guid isPermaLink="true">http://anyinlover.github.io/2016/04/21/em-algorithm/</guid>
        
        <category>机器学习算法</category>
        
        <category>Ng机器学习系列</category>
        
        
      </item>
    
      <item>
        <title>技术博客搭建</title>
        <description>&lt;p&gt;搭建一个属于自己的博客是我一直都有的梦想。作为一个有技术的人，当然应该选择gitpage。使用git来管理，用markdown来撰写，不担心内容丢失，也不用为繁杂的语法困扰，十分合我心意。下面就记录一下我的博客搭建过程，操作系统是OS X。&lt;/p&gt;

&lt;h2 id=&quot;gitpage&quot;&gt;申请gitpage账号，建立仓库。&lt;/h2&gt;
&lt;p&gt;想要用gitpage，首先得有个github账号。&lt;/p&gt;

&lt;h3 id=&quot;github&quot;&gt;github建立博客仓库&lt;/h3&gt;
&lt;p&gt;在github上建立一个仓库，仓库命名是有规范的。比如我的github用户名是anyinlover，那么仓库必须命名为anyinlover.github.io。&lt;/p&gt;

&lt;h3 id=&quot;section&quot;&gt;远程仓库同步到本地&lt;/h3&gt;
&lt;p&gt;找个合适的文件夹防止本地目录，比如Documents下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone anyinlover.github.io
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;假如还没有安装git，需要先装上git。&lt;/p&gt;

&lt;h2 id=&quot;jekyll&quot;&gt;安装jekyll并绑定文件夹&lt;/h2&gt;

&lt;h3 id=&quot;jekyll-1&quot;&gt;安装jekyll&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;gem install jekyll
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-1&quot;&gt;绑定文件夹&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;cd anyinlover.github.io
jekyll build
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-2&quot;&gt;安装配置模板&lt;/h2&gt;

&lt;p&gt;网上有挺多jekyll模板，除了&lt;a href=&quot;http://jekyllthemes.org&quot;&gt;官方模板网站&lt;/a&gt;，我更喜欢&lt;a href=&quot;https://drjekyllthemes.github.io&quot;&gt;dr. jekyll themes&lt;/a&gt;。如果要足够简单，&lt;a href=&quot;http://getpoole.com&quot;&gt;poole&lt;/a&gt;是个不错的选择，本次我使用的是一位中国人做的模板&lt;a href=&quot;https://github.com/Huxpro/huxpro.github.io&quot;&gt;hux&lt;/a&gt;，功能强大，在各方面都很趁意。&lt;/p&gt;

&lt;h3 id=&quot;section-3&quot;&gt;安装模板&lt;/h3&gt;

&lt;p&gt;从模板网站上把文件夹下载下来，解压到本地的博客文件夹。可以在本地打开jekyll先看一下，在浏览器输入0.0.0.0:4000：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;jekyll serve
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-4&quot;&gt;配置模板&lt;/h3&gt;

&lt;p&gt;配置模板是件比较烦的事情。特别是hux这个模板，虽然好看，但也意味着可配置的东西太多。配置文件主要是_config.xml这个文件，其他地方就需要修改html代码了。&lt;/p&gt;

&lt;h4 id=&quot;configxml&quot;&gt;配置_config.xml&lt;/h4&gt;
&lt;p&gt;前面的title啥的不必提了，看几个特别的地方。&lt;/p&gt;

&lt;p&gt;SNS设置那里我添加了豆瓣和简书的支持。增加douban_username和jianshu_username两行。&lt;/p&gt;

&lt;p&gt;anchorjs那里我把默认的true改成了false，因为正文中标题前莫名出现一个#看起来很难受，锚定的功能也用不上。&lt;/p&gt;

&lt;p&gt;评论系统我选择了disqus，注释了默认的duoshuo。&lt;/p&gt;

&lt;p&gt;kramdown那里作者配置了输入GFM，我也把它去掉了，我需要用latex语法输入数学公式，这是GFM不支持的。&lt;/p&gt;

&lt;h4 id=&quot;abouthtml&quot;&gt;配置about.html&lt;/h4&gt;
&lt;p&gt;作者把about.html直接写成了html，我觉得更合适的还是用md文件来表示。修改其中的description，删除秀恩爱照片。在正文中去掉了中英文版本（按钮略丑）。替换内容，只留下一段自己的介绍。&lt;/p&gt;

&lt;h4 id=&quot;indexhtmltagshtml&quot;&gt;配置index.html和tags.html&lt;/h4&gt;
&lt;p&gt;作者把这两页的描述都写进了html文件里，打开修改成自己要说的description,删除默认图片&lt;/p&gt;

&lt;h4 id=&quot;pagehtml&quot;&gt;配置page.html&lt;/h4&gt;
&lt;p&gt;在page.html中添加了豆瓣和简书的支持，在143行插入：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;li&amp;gt;
    &amp;lt;a target=&quot;_blank&quot; href=&quot;https://www.douban.com/people/48573787&quot;&amp;gt;
       &amp;lt;span class=&quot;fa-stack fa-lg&quot;&amp;gt;
            &amp;lt;i class=&quot;fa fa-circle fa-stack-2x&quot;&amp;gt;&amp;lt;/i&amp;gt;
            &amp;lt;i class=&quot;fa  fa-stack-1x fa-inverse&quot;&amp;gt;豆&amp;lt;/i&amp;gt;
       &amp;lt;/span&amp;gt;
     &amp;lt;/a&amp;gt;
&amp;lt;/li&amp;gt;


&amp;lt;li&amp;gt;
    &amp;lt;a target=&quot;_blank&quot; href=&quot;http://jianshu.com/users/33ab62821c57/timeline&quot;&amp;gt;
       &amp;lt;span class=&quot;fa-stack fa-lg&quot;&amp;gt;
            &amp;lt;i class=&quot;fa fa-circle fa-stack-2x&quot;&amp;gt;&amp;lt;/i&amp;gt;
            &amp;lt;i class=&quot;fa fa-stack-1x fa-inverse&quot;&amp;gt;简&amp;lt;/i&amp;gt;
       &amp;lt;/span&amp;gt;
    &amp;lt;/a&amp;gt;
&amp;lt;/li&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I
#### 配置footer.html
在footer.html中也要添加page.html里添加的代码。在最后的Theme by Hux，删去了github部分。&lt;/p&gt;

&lt;h2 id=&quot;section-5&quot;&gt;撰写博客并上传&lt;/h2&gt;

&lt;h3 id=&quot;section-6&quot;&gt;撰写&lt;/h3&gt;
&lt;p&gt;用macdown写博客，放在_posts文件夹下。需要有文件头类似如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;---
layout: post
title: &quot;技术博客搭建&quot;
subtitle: &quot;感谢gitpage，jekyll和hux模板&quot;
date: 2016-4-21
author: &quot;Anyinlover&quot;
catalog: true
tags:
  - 工具
---
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-7&quot;&gt;上传&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;git add *.md
git commit -m &quot;add somefile&quot;
git push origin master
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-8&quot;&gt;结尾&lt;/h2&gt;
&lt;p&gt;好啦，博客搭建过程大概如此，现在让我们一块欣赏吧。&lt;/p&gt;
</description>
        <pubDate>Thu, 21 Apr 2016 00:00:00 +0800</pubDate>
        <link>http://anyinlover.github.io/2016/04/21/build-the-blog/</link>
        <guid isPermaLink="true">http://anyinlover.github.io/2016/04/21/build-the-blog/</guid>
        
        <category>工具</category>
        
        
      </item>
    
      <item>
        <title>K均值聚类与混合高斯模型</title>
        <description>&lt;h2 id=&quot;k&quot;&gt;K均值聚类算法&lt;/h2&gt;
&lt;p&gt;在聚类问题中，我们给定一个训练集&lt;script type=&quot;math/tex&quot;&gt;\{x^{(1)},\cdots,x^{(m)}\}&lt;/script&gt;，标签&lt;script type=&quot;math/tex&quot;&gt;y^{(i)}&lt;/script&gt;没有给出，我们的目标还是把数据分类。这是一个非监督学习问题。&lt;/p&gt;

&lt;p&gt;K均值聚类算法给出了下面的方法：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;随机初始化聚类中心&lt;script type=&quot;math/tex&quot;&gt;\mu_1,\mu_2,\cdots,\mu_k \in \mathbb{R}^n&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;不断迭代直到收敛：{&lt;/p&gt;

    &lt;p&gt;对所有i，令：&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;c^{(i)} := \arg \min_j\|x^{(i)}-\mu_j\|^2&lt;/script&gt;

    &lt;p&gt;对所有j，令：&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\mu_j := \frac{\sum_{i=1}^m 1\{c^{(i)}=j\}x^{(i)}}{\sum_{i=1}^m 1\{c^{(i)}=j\}}&lt;/script&gt;

    &lt;p&gt;}&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;算法的内循环不停的在执行两个步骤，先把每个训练样本归给最近的聚类中心所代表的类，然后将某类的所有点计算平均值作为新的聚类中心。&lt;/p&gt;

&lt;p&gt;现在有一个问题，如何保证K均值聚类算法一定会收敛？定义失真函数为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(c,\mu)=\sum_{i=1}^m\|x^{(i)}-\mu_{c^{(i)}}\|^2&lt;/script&gt;

&lt;p&gt;失真函数定义了所有训练集离其聚类中心平方距离和。对于算法的内循环的两步而言，每一步都是朝着J下降的方向进行。实际上K均值聚类算法就是一类坐标下降法。最终J会达到收敛。&lt;/p&gt;

&lt;p&gt;失真函数J是一个非凸函数，所以坐标下降法不能保证J能收敛到全局最小值。为了不限于局部最小值，可以多次跑K均值聚类算法（使用不同的随机初始值），取最低失真函数。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;混合高斯模型&lt;/h2&gt;
&lt;p&gt;同样假设下面的非监督学习问题，给定一组训练集&lt;script type=&quot;math/tex&quot;&gt;\{x^{(i)},\cdots,x^{(m)}\}&lt;/script&gt;，没有标签。&lt;/p&gt;

&lt;p&gt;我们用一个联合分布&lt;script type=&quot;math/tex&quot;&gt;p(x^{(i)},z^{(i)})=p(x^{(i)} \mid z^{(i)})p(z^{(i)})&lt;/script&gt;来对数据进行建模。这里，&lt;script type=&quot;math/tex&quot;&gt;z^{(i)} \sim \text{Multinomial}(\phi)（\phi_j \geq 0, \sum_{j=1}^k \phi_j = 1, \phi_j=p(z^{(i)}=j))&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;x^{(i)} \mid z^{(i)}=j \sim \mathcal{N}(\mu_j,\Sigma_j)&lt;/script&gt;。因此我们的模型是随机从&lt;script type=&quot;math/tex&quot;&gt;\{1,\cdots,k\}&lt;/script&gt;中选择&lt;script type=&quot;math/tex&quot;&gt;z^{(i)}&lt;/script&gt;，然后从对应&lt;script type=&quot;math/tex&quot;&gt;z^{(i)}&lt;/script&gt;的高斯分布中选择&lt;script type=&quot;math/tex&quot;&gt;x^{(i)}&lt;/script&gt;。这被称为混合高斯模型。此外由于&lt;script type=&quot;math/tex&quot;&gt;z^{(i)}&lt;/script&gt;是潜在的随机变量，这使我们的评估变得困难。&lt;/p&gt;

&lt;p&gt;混合高斯模型的参数有&lt;script type=&quot;math/tex&quot;&gt;\phi,\mu,\Sigma&lt;/script&gt;，最大似然函数是：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
\ell(\phi,\mu,\Sigma) &amp;= \sum_{i=1}^m \log p(x^{(i)};\phi,\mu,\Sigma) \\
&amp;= \sum_{i=1}^m \log \sum_{z^{(i)}=1}^k p(x^{(i)}\mid z^{(i)};\mu,\Sigma)p(z^{(i)};\phi)
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;然而上式无法用梯度来计算，因此也无法解得最大似然估计。&lt;/p&gt;

&lt;p&gt;注意上式中&lt;script type=&quot;math/tex&quot;&gt;\sum_{z^{(i)}=1}^k&lt;/script&gt;产生的原因在于&lt;script type=&quot;math/tex&quot;&gt;z^{(i)}&lt;/script&gt;是未知的，假如&lt;script type=&quot;math/tex&quot;&gt;z^{(i)}&lt;/script&gt;是已知的，那么最大似然函数可以简化为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\ell(\phi,\mu,\Sigma)=\sum_{i=1}^m\log p(x^{(i)}\mid z^{(i)};\mu,\Sigma)+p(z^{(i)};\phi)&lt;/script&gt;

&lt;p&gt;做最大似然估计可以解出：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
\phi_j &amp;= \frac{1}{m} \sum_{i=1}^m 1 \{z^{(i)}=j\}  \\
\mu_j &amp;= \frac{\sum_{i=1}^m 1\{z^{(i)}=j\}x^{(i)}}{\sum_{i=1}^m 1\{z^{(i)}=j\}} \\
\Sigma_j &amp;= \frac{\sum_{i=1}^m 1\{z^{(i)}=j\}(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T}{\sum_{i=1}^m 1\{z^{(i)}=j\}}
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;事实上，假如&lt;script type=&quot;math/tex&quot;&gt;z^{(i)}&lt;/script&gt;是已知的，那么最大似然估计和之前的高斯判别分析非常类似，只是&lt;script type=&quot;math/tex&quot;&gt;z^{(i)}&lt;/script&gt;表示了标签。&lt;/p&gt;

&lt;p&gt;然而&lt;script type=&quot;math/tex&quot;&gt;z^{(i)}&lt;/script&gt;是未知的，这里就引出了最大期望算法。最大期望算法主要由两步构成，E步尝试猜测&lt;script type=&quot;math/tex&quot;&gt;z^{(i)}&lt;/script&gt;的值，M步根据我们猜测的&lt;script type=&quot;math/tex&quot;&gt;z^{(i)}&lt;/script&gt;更新模型参数。&lt;/p&gt;

&lt;p&gt;迭代直到收敛：{&lt;/p&gt;

&lt;p&gt;（E步）对每个i,j，令：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\omega_j^{(i)} := p(z^{(i)}=j\mid x^{(i)}; \phi, \mu, \Sigma)&lt;/script&gt;

&lt;p&gt;（M步）更新参数：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
\phi_j &amp;:= \frac{1}{m} \sum_{i=1}^m \omega_j^{(i)}  \\
\mu_j &amp;:= \frac{\sum_{i=1}^m \omega_j^{(i)}x^{(i)}}{\sum_{i=1}^m \omega_j^{(i)}} \\
\Sigma_j &amp;= \frac{\sum_{i=1}^m \omega_j^{(i)}(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T}{\sum_{i=1}^m \omega_j^{(i)}}
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;在E步，我们通过计算后验概率来得到：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(z^{(i)}=j\mid x^{(i)}; \phi, \mu, \Sigma)=\frac{p(x^{(i)}|z^{(i)}=j;\mu,\Sigma)p(z^{(i)}=j;\phi)}{\sum_{l=1}^kp(x^{(i)}|z^{(i)}=l;\mu,\Sigma)p(z^{(i)}=l;\phi)}&lt;/script&gt;

&lt;p&gt;EM算法和K均值聚类算法有很多相似的地方，都是两步走，都是可能陷入局部最优解，需要多次不同的初始值进行计算。&lt;/p&gt;

&lt;p&gt;EM算法是否确保收敛等性质放到下一章证明。&lt;/p&gt;
</description>
        <pubDate>Mon, 18 Apr 2016 00:00:00 +0800</pubDate>
        <link>http://anyinlover.github.io/2016/04/18/K%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB%E4%B8%8E%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/</link>
        <guid isPermaLink="true">http://anyinlover.github.io/2016/04/18/K%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB%E4%B8%8E%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/</guid>
        
        <category>Ng机器学习系列</category>
        
        <category>机器学习算法</category>
        
        
      </item>
    
      <item>
        <title>规则化与模型选择</title>
        <description>&lt;p&gt;对于有限个模型&lt;script type=&quot;math/tex&quot;&gt;\mathcal{M}=\{M_1,\cdots,M_d\}&lt;/script&gt;，我们如何选择最优的模型。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;交叉验证&lt;/h2&gt;
&lt;p&gt;对于整个训练集进行训练，从中挑选最小训练误差来评价模型并不是一个好方法，因为这样往往选出高方差的模型。&lt;/p&gt;

&lt;p&gt;因此有下面的交叉验证法：把训练数据分S成&lt;script type=&quot;math/tex&quot;&gt;S_{train}&lt;/script&gt;和&lt;script type=&quot;math/tex&quot;&gt;S_{cv}&lt;/script&gt;两部分，一般分成七三开。&lt;script type=&quot;math/tex&quot;&gt;S_{train}&lt;/script&gt;用来训练数据，&lt;script type=&quot;math/tex&quot;&gt;S_{cv}&lt;/script&gt;用来验证数据。从中挑出验证误差最小的模型作为最优模型，再进行全数据集重训练。&lt;/p&gt;

&lt;p&gt;交叉验证避免了一味选择复杂的模型，但也造成了数据的浪费，特别在数据量不足时可能影响模型的训练精度。下面是一种更改良的方法，称为k折交叉验证。&lt;/p&gt;

&lt;p&gt;把数据集S分为k份，在评估某一个模型时，每一次将其中一份作为验证集，其他k-1份作为训练集，计算出平均的验证误差。最后挑选出最优模型进行全数据集重训练。&lt;/p&gt;

&lt;p&gt;交叉验证除了使用于模型选择上，还可用以评估单个模型的预测准确度上。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;特征选择&lt;/h2&gt;
&lt;p&gt;有些时候会遇到特征过多的情况，这种情况下很容易造成过拟合。这时我们就需要采取特征选择算法来降低特征的数量。&lt;/p&gt;

&lt;p&gt;一种算法称为前向搜索。从零开始，每次增加一个特征，利用交叉验证计算误差，选择误差最小的增加，直到达到阈值。&lt;/p&gt;

&lt;p&gt;与之相对的另一种算法称为后向搜索。这两种算法的时间复杂度都比较高，需要&lt;script type=&quot;math/tex&quot;&gt;O(n^2)&lt;/script&gt;。&lt;/p&gt;

&lt;p&gt;第二种算法称为过滤特征选择。本质就是简单计算每个特征&lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt;和&lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;的相关度&lt;script type=&quot;math/tex&quot;&gt;S(i)&lt;/script&gt;。选择得分最高的k个特征。&lt;/p&gt;

&lt;p&gt;在实际中，最常见的就是用互信息&lt;script type=&quot;math/tex&quot;&gt;MI(x_i,y)&lt;/script&gt;来衡量相关度：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;MI(x_i,y)=\sum_{x_i \in \{0,1\}} \sum_{y \in \{0,1\}} p(x_i,y) \log \frac{p(x_i,y)}{p(x_i)p(y)}&lt;/script&gt;

&lt;p&gt;互信息还可以用KL距离来表示:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;MI(x_i,y)=KL(p(x_i,y)\|p(x_i)p(y))&lt;/script&gt;

&lt;p&gt;最后一点是如何确定k的取值，标准做法就是采用交叉验证。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;贝叶斯统计和规则化&lt;/h2&gt;
&lt;p&gt;回顾根据最大似然函数选择参数：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_{ML}=\arg \max_{\theta} \prod_{i=1}^m p(y^{(i)}\mid x^{(i)}; \theta)&lt;/script&gt;

&lt;p&gt;从频率学派角度来看，&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;不是随机的，而是确定而未知的值。我们的任务就是通过统计方法（比如最大似然函数）来找出它的值。&lt;/p&gt;

&lt;p&gt;从贝叶斯学派角度来看，&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;是随机未知的值，我们可以给定一个先验分布&lt;script type=&quot;math/tex&quot;&gt;p(\theta)&lt;/script&gt;，给定训练集&lt;script type=&quot;math/tex&quot;&gt;S=\{(x^{(i)},y^{(i)})\}_{i=1}^m&lt;/script&gt;，我们可以计算后验概率：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
p(\theta\mid S) &amp;= \frac{p(S\mid \theta)p(\theta)}{p(S)} \\
&amp;= \frac{(\prod_{i=1}^mp(y^{(i)}\mid x^{(i)},\theta))p(\theta)}{\int_\theta(\prod_{i=1}^mp(y^{(i)}\mid x^{(i)},\theta))p(\theta)d\theta}
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;这里的&lt;script type=&quot;math/tex&quot;&gt;p(y^{(i)}\mid x^{(i)},\theta)&lt;/script&gt;可以从学习模型中得到。比如对于贝叶斯逻辑回归而言，&lt;script type=&quot;math/tex&quot;&gt;p(y^{(i)}\mid x^{(i)},\theta)=h_\theta(x^{(i)})^{y^{(i)}}(1-h_\theta(x^{(i)}))^{(1-y^{(i)})}&lt;/script&gt;，其中&lt;script type=&quot;math/tex&quot;&gt;h_\theta(x^{(i)})=1/(1+\exp(-\theta^Tx^{(i)}))。&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;给定新的测试样本，我们可以计算对于标签的后验概率：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(y|x,S)=\int_\theta p(y\mid x,\theta)p(\theta\mid S)d\theta&lt;/script&gt;

&lt;p&gt;但事实上后验概率公式中的分母积分很难计算，我们常常最大后验概率估计代替，只比最大似然估计函数多了一个&lt;script type=&quot;math/tex&quot;&gt;p(\theta)&lt;/script&gt;：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_{MAP}=\arg \max_\theta \prod_{i=1}^mp(y^{(i)}\mid x^{(i)},\theta))p(\theta)&lt;/script&gt;

&lt;p&gt;在实际应用中,常常假定先验概率&lt;script type=&quot;math/tex&quot;&gt;p(\theta)&lt;/script&gt;满足&lt;script type=&quot;math/tex&quot;&gt;\theta \sim \mathcal{N}(0, \tau^2I)&lt;/script&gt;。贝叶斯最大后验概率估计往往比最大似然估计更容易克服过拟合问题。&lt;/p&gt;
</description>
        <pubDate>Sun, 17 Apr 2016 00:00:00 +0800</pubDate>
        <link>http://anyinlover.github.io/2016/04/17/%E8%A7%84%E5%88%99%E5%8C%96%E4%B8%8E%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9/</link>
        <guid isPermaLink="true">http://anyinlover.github.io/2016/04/17/%E8%A7%84%E5%88%99%E5%8C%96%E4%B8%8E%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9/</guid>
        
        <category>Ng机器学习系列</category>
        
        <category>机器学习理论</category>
        
        
      </item>
    
      <item>
        <title>机器学习原理</title>
        <description>&lt;h2 id=&quot;section&quot;&gt;偏差和方差&lt;/h2&gt;
&lt;p&gt;还是从房价预测的例子入手，我们可以分别用一次函数，三次函数和五次函数去拟合测试集。但不是三个模型都是好模型，第一个和第三个都存在着泛化误差，即测试集的拟合误差要大于训练集的期望误差。&lt;/p&gt;

&lt;p&gt;对于一次函数模型，我们认为模型有偏差，即使给出足够大的训练集也会使数据欠拟合。而对于五次函数模型，我们认为模型有方差，仅仅是很好的吻合了小范围的测试集，对数据过拟合，不能准确反映更一般的输入输出关系。&lt;/p&gt;

&lt;p&gt;很多时候都需要在偏差和方差之间做权衡。假如我们的模型过于简单只有很少的几个参数，那很可能有大的偏差；假如模型过于复杂有很多的参数，那可能会有大的方差。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;预备知识&lt;/h2&gt;
&lt;p&gt;我们开始学习一些机器学习原理中最基石的规则。最终希望能够回答三个问题：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;我们是否可以把偏差和方差公式化？最终引出模型选择方法。&lt;/li&gt;
  &lt;li&gt;为何从测试集中可以获得泛化误差？测试集误差和泛化误差是否有联系？&lt;/li&gt;
  &lt;li&gt;是否能在某些条件下证明算法一定有效？&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;首先介绍两个引理（这里没有证明，可以直观感受）。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;联合界定理&lt;/strong&gt;：存在&lt;script type=&quot;math/tex&quot;&gt;A_1,A_2,\cdots,A_k&lt;/script&gt;共k个不同事件（可能独立也可能非独立），必然有：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(A_1 \cup \cdots \cup A_k) \leq P(A_1)+\cdots + P(A_k)&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;霍夫丁不等式&lt;/strong&gt;：令&lt;script type=&quot;math/tex&quot;&gt;Z_1,\cdots,Z_m&lt;/script&gt;是m个服从伯努利分布的独立同分布随机变量，令&lt;script type=&quot;math/tex&quot;&gt;\hat{\phi}=(1/m)\sum_{i=1}^m Z_i&lt;/script&gt;作为随机变量的均值，令任意&lt;script type=&quot;math/tex&quot;&gt;\gamma&gt;0&lt;/script&gt;固定，有下面的关系：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(|\phi - \hat{\phi}| &gt; \gamma) \leq 2\exp(-2\gamma^2m)&lt;/script&gt;

&lt;p&gt;为了简化解释，我们再以二元分类为例。假设给定一个训练集&lt;script type=&quot;math/tex&quot;&gt;S=\{(x^{(i)},y^{(i)};i=1,\cdots,m\}&lt;/script&gt;，训练样本&lt;script type=&quot;math/tex&quot;&gt;(x^{(i)},y^{(i)})&lt;/script&gt;满足可能性分布&lt;script type=&quot;math/tex&quot;&gt;\mathcal{D}&lt;/script&gt;，对于假说h，定义训练误差（经验误差）为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\epsilon}(h)=\frac{1}{m} \sum_{i=1}^m1\{h(x^{(i)} \neq y^{(i)}\}&lt;/script&gt;

&lt;p&gt;定义泛化误差为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\epsilon(h)=P_{(x,y) \sim \mathcal{D}}(h(x) \neq y)&lt;/script&gt;

&lt;p&gt;PAC是一组构建机器学习原理的假定。其中最重要的两条就是训练集和测试集满足同分布，训练样本具备独立性。&lt;/p&gt;

&lt;p&gt;考虑线性分类，令&lt;script type=&quot;math/tex&quot;&gt;h_\theta(x)=1\{\theta^Tx \geq 0\}&lt;/script&gt;，评估参数&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;拟合的一种方式就是让训练误差最小化：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\theta} = \arg \min_{\theta} \hat{\epsilon}(h_{\theta})&lt;/script&gt;

&lt;p&gt;这个过程被称为经验风险最小化，它被视为是最基础的学习算法。ERM本身是非凸不能用一般优化算法求解的，逻辑回归和支持向量机被看做对这种算法的凸性近似。&lt;/p&gt;

&lt;p&gt;更一般化，我们用假设集&lt;script type=&quot;math/tex&quot;&gt;\mathcal{H}&lt;/script&gt;来定义一组分类器。比如对于线性分类法，&lt;script type=&quot;math/tex&quot;&gt;\mathcal{H}=\{h_{\theta}: h_{\theta}(x)=1\{\theta^Tx \geq 0\}, \theta \in \mathbb{R}^{n+1}\}&lt;/script&gt;。经验风险最小化可以写成下式：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{h}=\arg \min_{h \in \mathcal{H}} \hat{\epsilon}(h)&lt;/script&gt;

&lt;h2 id=&quot;section-2&quot;&gt;有限假设集&lt;/h2&gt;
&lt;p&gt;我们先来考虑假说集有限的情况，即&lt;script type=&quot;math/tex&quot;&gt;\mathcal{H}=\{h_1,\cdots,h_k\}&lt;/script&gt;，假设集由k个假说构成。经验风险最小化算法选择其中使训练误差最小的假说作为&lt;script type=&quot;math/tex&quot;&gt;\hat{h}&lt;/script&gt;。&lt;/p&gt;

&lt;p&gt;考虑一个伯努利随机变量Z，样本&lt;script type=&quot;math/tex&quot;&gt;(x,y) \sim \mathcal{D}&lt;/script&gt;，令&lt;script type=&quot;math/tex&quot;&gt;Z=1\{h_i(x) \neq y\}&lt;/script&gt;。对训练样本我们同样定义&lt;script type=&quot;math/tex&quot;&gt;Z_j=1\{h_i(x^{(j)}) \neq y^{(j)}\}&lt;/script&gt;。训练样本和测试样本服从同分布。&lt;/p&gt;

&lt;p&gt;可以看到误分类的可能性&lt;script type=&quot;math/tex&quot;&gt;\epsilon(h)&lt;/script&gt;就等于&lt;script type=&quot;math/tex&quot;&gt;Z(Z_j)&lt;/script&gt;的期望值，此外，训练误差可表示成：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\epsilon}(h_i)=\frac{1}{m}\sum_{j=1}^m Z_j&lt;/script&gt;

&lt;p&gt;因此我们在这里可以应用霍夫丁不等式：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(|\epsilon(h_i)-\hat{\epsilon}(h_i)| &gt; \gamma) \leq 2\exp(-2\gamma^2m)&lt;/script&gt;

&lt;p&gt;这显示了当m足够大时，对特定的&lt;script type=&quot;math/tex&quot;&gt;h_i&lt;/script&gt;训练误差有极高的可能性与泛化误差接近。下面我们要证明对于所有&lt;script type=&quot;math/tex&quot;&gt;h \in \mathcal{H}&lt;/script&gt;，上面的特性也成立。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
P(\exists h \in \mathcal{H}.|\epsilon(h_i)-\hat{\epsilon}(h_i)| &gt; \gamma) &amp;= P(A_1 \cup \cdots \cup A_k) \\
&amp;\leq \sum_{i=1}^k P(A_i) \\
&amp;\leq \sum_{i=1}^k 2 \exp(-2\gamma^2m) \\
&amp;=2k \exp(-2\gamma^2m)
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;这个结果被称为一致收敛，对于所有h都满足。&lt;/p&gt;

&lt;p&gt;令&lt;script type=&quot;math/tex&quot;&gt;\delta=2k\exp(-2\gamma^2m)&lt;/script&gt;，我们可以计算出为达到概率在&lt;script type=&quot;math/tex&quot;&gt;1-\delta&lt;/script&gt;，精确度在&lt;script type=&quot;math/tex&quot;&gt;\pm\gamma&lt;/script&gt;内要求所需的样本复杂度m：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; m \geq \frac{1}{2\gamma^2} \log \frac{2k}{\delta}&lt;/script&gt;

&lt;p&gt;同样，我们也可以求得精确度：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;|\hat{\epsilon}(h)-\epsilon(h)| \leq \sqrt{\frac{1}{2m} \log{\frac{2k}{\delta}}}&lt;/script&gt;

&lt;p&gt;现在还有一个问题，模型的泛化误差和最小训练误差&lt;script type=&quot;math/tex&quot;&gt;\hat{h}=\arg \min_{h \in \mathcal{H}} \hat{\epsilon}(h)&lt;/script&gt;存在什么联系？&lt;/p&gt;

&lt;p&gt;令&lt;script type=&quot;math/tex&quot;&gt;h^*=\arg \min_{h \in \mathcal{H}} \epsilon(h)&lt;/script&gt;作为假设集&lt;script type=&quot;math/tex&quot;&gt;\mathcal{H}&lt;/script&gt;中最好的一个，它与训练误差最小的假设存在以下关系：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
\epsilon(\hat{h}) &amp;\leq \hat{\epsilon}(\hat{h})+\gamma \\
&amp;\leq \hat{\epsilon}(h^*)+\gamma \\
&amp;\leq \epsilon(h^*)+2\gamma
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;在&lt;script type=&quot;math/tex&quot;&gt;\mid \mathcal{H}\mid =k&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;m,\delta&lt;/script&gt;固定，至少有&lt;script type=&quot;math/tex&quot;&gt;1-\delta&lt;/script&gt;的可能性：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\epsilon(\hat{h}) \leq (\min_{h\in\mathcal{H}} \epsilon(h)) + 2 \sqrt{\frac{1}{2m} \log \frac{2k}{\delta}}&lt;/script&gt;

&lt;p&gt;这也从另一面证明了偏差和方差的矛盾性。假设我们的假设集扩大了，则前一项下降，后一项增加。反之亦然。&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;无限假设集&lt;/h2&gt;
&lt;p&gt;在有限假设集中我们得出了一些有用的定理。但对于参数是实数的假设集来说，有无限个假设。我们是否能得出类似的结论？&lt;/p&gt;

&lt;p&gt;首先做一个不是很正确的解释。因为实数在计算机中也是由有限位组成，因此所谓的无限假设实际上也是有限的，因此可以套用上一节的结论来处理。&lt;/p&gt;

&lt;p&gt;为得出无限假设集的结果，我们需要定义VC维。给定样本点集合&lt;script type=&quot;math/tex&quot;&gt;S=\{x^{(i)},\cdots,x^{(d)}\}, x^{(i)} \in \mathcal{X}&lt;/script&gt;，我们说&lt;script type=&quot;math/tex&quot;&gt;\mathcal{H}&lt;/script&gt;粉碎&lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt;假如&lt;script type=&quot;math/tex&quot;&gt;\mathcal{H}&lt;/script&gt;可以识别任意的标签。我们定义VC维，&lt;script type=&quot;math/tex&quot;&gt;VC(\mathcal{H})&lt;/script&gt;是最大的可粉碎样本大小。例如对于有两个维度的线性分类而言，&lt;script type=&quot;math/tex&quot;&gt;VC(\mathcal{H})=3&lt;/script&gt;。&lt;/p&gt;

&lt;p&gt;下式展示了对于无限假设集的定理，对于给定&lt;script type=&quot;math/tex&quot;&gt;\mathcal{H}&lt;/script&gt;，令&lt;script type=&quot;math/tex&quot;&gt;d=VC(\mathcal{H})&lt;/script&gt;，至少有&lt;script type=&quot;math/tex&quot;&gt;1-\delta&lt;/script&gt;的可能性：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;|\epsilon(h)-\hat{\epsilon}(h)| \leq O\left(\sqrt{\frac{d}{m} \log \frac{m}{d} + \frac{1}{m} \log \frac{1}{\delta}}\right)&lt;/script&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;同样还有下面的结论：对$$&lt;/td&gt;
      &lt;td&gt;\epsilon(h)-\hat{\epsilon}(h)&lt;/td&gt;
      &lt;td&gt;\leq \gamma&lt;script type=&quot;math/tex&quot;&gt; 要求至少&lt;/script&gt;1-\delta&lt;script type=&quot;math/tex&quot;&gt;概率对所有&lt;/script&gt;h \in \mathcal{H}&lt;script type=&quot;math/tex&quot;&gt;成立，需要满足&lt;/script&gt;m=O_{\gamma,\delta}(d)$$。即对于最小化训练误差的算法，所需的训练样本数和算法参数个数几乎成线性关系。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
</description>
        <pubDate>Sun, 17 Apr 2016 00:00:00 +0800</pubDate>
        <link>http://anyinlover.github.io/2016/04/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8E%9F%E7%90%86/</link>
        <guid isPermaLink="true">http://anyinlover.github.io/2016/04/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8E%9F%E7%90%86/</guid>
        
        <category>Ng机器学习系列</category>
        
        <category>机器学习理论</category>
        
        
      </item>
    
      <item>
        <title>支持向量机</title>
        <description>&lt;p&gt;这一章主要学习支持向量机。支持向量机是目前最好的监督学习算法之一。但支持向量机本身难度较大，需要较强的数学基础。假如抛开数学部分，其本质就是用拉格朗日对偶法计算最大间隔，利用核函数简化高维映射的计算，用SMO算法更新参数。本文也将根据NG讲义的这一顺序展开。此外，&lt;a href=&quot;http://blog.csdn.net/macyang/article/details/38782399&quot;&gt;这里&lt;/a&gt;有一份比较完善的中文入门资料。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;间隔入门&lt;/h2&gt;
&lt;p&gt;回顾之前的逻辑回归，&lt;script type=&quot;math/tex&quot;&gt;h_\theta(x)=g(\theta^Tx)&lt;/script&gt;，当&lt;script type=&quot;math/tex&quot;&gt;\theta^Tx \geq 0&lt;/script&gt;时预测为1，否则为0。这里存在一个预测的可信度问题，假如&lt;script type=&quot;math/tex&quot;&gt;\theta^Tx \gg 0&lt;/script&gt;或者&lt;script type=&quot;math/tex&quot;&gt;\theta^Tx \ll 0&lt;/script&gt;，那么预测的可信度就高。换一个角度，预测点离分割面越远，预测可信度就越高。把训练集离分割面最近的点到分割面的距离称为间隔，支持向量机的使命就是找到有最大间隔得分割面，使预测可信度提到最高。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;标记&lt;/h2&gt;
&lt;p&gt;为了讨论支持向量机，需要引入一套新的标记符号。&lt;script type=&quot;math/tex&quot;&gt;y\in \{-1,1\}&lt;/script&gt;而不再是&lt;script type=&quot;math/tex&quot;&gt;\{0,1\}&lt;/script&gt;。分类器的表示也发生了变化：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_{\omega,b}(x)=g(\omega^Tx+b)&lt;/script&gt;

&lt;p&gt;这里用的g是感知机算法，即&lt;script type=&quot;math/tex&quot;&gt;g(z)=1,z\geq0&lt;/script&gt;，否则取-1。b是单独的截距，即之前的&lt;script type=&quot;math/tex&quot;&gt;\theta_0&lt;/script&gt;。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;函数间隔和几何间隔&lt;/h2&gt;
&lt;p&gt;根据一个训练样本&lt;script type=&quot;math/tex&quot;&gt;(x^{(i)},y^{(i)})&lt;/script&gt;，定义函数间隔如下：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\gamma}^{(i)}=y^{(i)}(\omega^Tx+b)&lt;/script&gt;

&lt;p&gt;函数间隔越大，可信度就越高。&lt;/p&gt;

&lt;p&gt;给定训练集&lt;script type=&quot;math/tex&quot;&gt;S=\{(x^{(i)},y^{(i)});i=1,\cdots,m\}&lt;/script&gt;，定义函数间隔是所有函数间隔中最小的那个：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\gamma}=\min_{i=1,\cdots,m}\hat{\gamma}^{(i)}&lt;/script&gt;

&lt;p&gt;下面的问题是如何找出&lt;script type=&quot;math/tex&quot;&gt;\gamma^{(i)}&lt;/script&gt;的表达式，分割面与&lt;script type=&quot;math/tex&quot;&gt;\omega&lt;/script&gt;永远是正交的。对于训练样本在分割面上的投影，可以表示成 &lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x^{(i)}-\gamma^{(i)} \cdot \frac{\omega}{\|\omega\|}&lt;/script&gt;

&lt;p&gt;又因为投影点在分割面上，所以有：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\omega^T(x^{(i)}-\gamma^{(i)}\frac{\omega}{\|\omega\|})+b=0&lt;/script&gt;

&lt;p&gt;解得：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\gamma^{(i)}=\frac{\omega^Tx^{(i)}+b}{\|\omega\|}=(\frac{\omega}{\|\omega\|})^Tx^{(i)}+\frac{b}{\|\omega\|}&lt;/script&gt;

&lt;p&gt;结合y的取值可以得到几何间隔：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\gamma^{(i)}=y^{(i)}((\frac{\omega}{\|\omega\|})^Tx^{(i)}+\frac{b}{\|\omega\|})&lt;/script&gt;

&lt;p&gt;我们发现几何间隔与函数间隔相比只是除了&lt;script type=&quot;math/tex&quot;&gt;\|\omega\|&lt;/script&gt;，当&lt;script type=&quot;math/tex&quot;&gt;\|\omega\|=1&lt;/script&gt;时函数间隔等于几何间隔。而且几何间隔不会受到比例系数的影响。
## 最优间隔分类器&lt;/p&gt;

&lt;p&gt;对于一个可以线性分割的训练集来说，现在我们的任务就是要找到一个最大几何间隔。问题就转化成下面的优化问题：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
\max_{\gamma,\omega,b}  \quad  &amp;\gamma \\
\text{s.t.}  \quad &amp; y^{(i)}(\omega^Tx^{(i)}+b)\geq \gamma, i=1,\cdots,m \\
&amp; \|\omega\|=1
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;对于&lt;script type=&quot;math/tex&quot;&gt;\|\omega\|=1&lt;/script&gt;这样的约束条件很难处理，把几何间隔用函数间隔替换了，优化问题转化为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
\max_{\hat{\gamma},\omega,b}  \quad  &amp; \frac{\hat{\gamma}}{\|\omega\|} \\
\text{s.t.}  \quad &amp; y^{(i)}(\omega^Tx^{(i)}+b)\geq \hat{\gamma}, i=1,\cdots,m 
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;进一步，利用&lt;script type=&quot;math/tex&quot;&gt;\hat{\gamma}&lt;/script&gt;的可伸缩性质，令&lt;script type=&quot;math/tex&quot;&gt;\hat{\gamma}=1&lt;/script&gt;，求&lt;script type=&quot;math/tex&quot;&gt;1/\|\omega\|&lt;/script&gt;相当于求&lt;script type=&quot;math/tex&quot;&gt;\|\omega\|^2&lt;/script&gt;的最小值。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
\min_{\omega,b}  \quad  &amp; \frac{1}{2}\|\omega\|^2 \\
\text{s.t.}  \quad &amp; y^{(i)}(\omega^Tx^{(i)}+b)\geq 1, i=1,\cdots,m 
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;现在优化问题转变成了一个凸二次函数带一个线性约束，这样的优化问题可以用QP软件来处理。&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;拉格朗日对偶&lt;/h2&gt;
&lt;p&gt;我们到了支持向量机的难点之一，拉格朗日对偶。这其实就是一种把多元约束优化问题转换为一元约束优化问题的思想。
考虑下面这样的问题形式：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
\min_\omega \quad &amp; f(\omega) \\
\text{s.t.} \quad &amp; h_i(\omega)=0, i=1,\cdots,l
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;我们定义拉格朗日公式为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{L}(\omega,\beta)=f(\omega)+\sum_{i=1}^l \beta_ih_i(\omega)&lt;/script&gt;

&lt;p&gt;这里&lt;script type=&quot;math/tex&quot;&gt;\beta_i&lt;/script&gt;被称为拉格朗日算子。通过求偏导可以求解得到&lt;script type=&quot;math/tex&quot;&gt;\omega&lt;/script&gt;和&lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial\mathcal{L}}{\partial\omega_i}=0; \frac{\partial \mathcal{L}}{\partial\beta_i}=0&lt;/script&gt;

&lt;p&gt;将上面的形式进行拓展，把不等约束也包含在内，考虑下面的问题形式，称为原始优化问题：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
\min_\omega \quad &amp; f(\omega) \\
\text{s.t.} \quad &amp; g_i(\omega) \leq 0, i=1,\cdots,k \\
&amp; h_i(\omega)=0, i=1,\cdots,l
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;为了解决上述问题，我们定义了一般化的拉格朗日方程：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{L}(\omega,\alpha,\beta)=f(\omega)+\sum_{i=1}^k\alpha_ig_i(\omega)+\sum_{i=1}^l\beta_ih_i(\omega)&lt;/script&gt;

&lt;p&gt;考虑以下方程：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_{\mathcal{p}}(\omega)=\max_{\alpha,\beta:\alpha_i \geq 0}\mathcal{L}(\omega,\alpha,\beta)&lt;/script&gt;

&lt;p&gt;假如&lt;script type=&quot;math/tex&quot;&gt;g_i(\omega)&gt;0&lt;/script&gt; 或者&lt;script type=&quot;math/tex&quot;&gt;h_i(\omega) \neq 0&lt;/script&gt;，都会使得&lt;script type=&quot;math/tex&quot;&gt;\theta_{\mathcal{p}}(\omega)=\infty&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\theta_{\mathcal{p}}(\omega)=
\begin{cases}
f(\omega) \quad &amp; \text{if }\omega \text{ satisfies primal constraints}\\
\infty \quad &amp; \text{otherwise}
\end{cases}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;考虑最小化问题：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_{\omega} \theta_{\mathcal{p}}(\omega)=\min_{\omega} \max_{\alpha,\beta:\alpha_i \geq 0} \mathcal{L}(\omega,\alpha,\beta)&lt;/script&gt;

&lt;p&gt;这就是我们的原始优化问题。最后定义优化值&lt;script type=&quot;math/tex&quot;&gt;p^*=\min_{\omega} \theta_p(\omega)&lt;/script&gt;，称为原始问题的值。&lt;/p&gt;

&lt;p&gt;再来看一个有些不同的问题。定义：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_{\mathcal{D}}(\alpha,\beta)=\min_{\omega} \mathcal{L}(\omega,\alpha,\beta)&lt;/script&gt;

&lt;p&gt;我们可以得到对偶优化问题：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\max_{\alpha,\beta:\alpha_i \geq 0} \theta_{\mathcal{D}}(\alpha,\beta)=\max_{\alpha,\beta:\alpha_i \geq 0} \min_{\omega} \mathcal{L}(\omega,\alpha,\beta)&lt;/script&gt;

&lt;p&gt;对偶优化问题相当于和初始优化问题交换了max和min的顺序，我们定义对偶优化问题的优化值为&lt;script type=&quot;math/tex&quot;&gt;d^*=\max_{\alpha,\beta:\alpha_i \geq 0} \theta_{\mathcal{D}} (\omega)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;关于max和min有下面的结论：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\max_x \min_y f(x,y) \leq \min_y \max_x f(x,y)&lt;/script&gt;

&lt;p&gt;简要证明：对于任意x,y，下式成立：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_s f(x,s) \leq f(x,y) \leq\max_tf(t,y)&lt;/script&gt;

&lt;p&gt;因此下式成立，得到证明：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; \max_x \min_s f(x,s) \leq \min_y \max_t f(t,y)&lt;/script&gt;

&lt;p&gt;回到我们的拉格朗日对偶问题：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;d^*=\max_{\alpha,\beta:\alpha_i \geq 0} \min_{\omega} \mathcal{L}(\omega,\alpha,\beta) \leq \min_{\omega} \max_{\alpha,\beta:\alpha_i \geq 0} \mathcal{L}(\omega,\alpha,\beta) = p^*&lt;/script&gt;

&lt;p&gt;在满足一定的条件下，上式中等号成立，因此我们可以用求解对偶问题来求解原始问题。&lt;/p&gt;

&lt;p&gt;假设&lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;和&lt;script type=&quot;math/tex&quot;&gt;g_i&lt;/script&gt;是凸函数，&lt;script type=&quot;math/tex&quot;&gt;h_i&lt;/script&gt;是线性的，约束&lt;script type=&quot;math/tex&quot;&gt;g_i&lt;/script&gt;是可满足的。那必然会存在&lt;script type=&quot;math/tex&quot;&gt;\omega^*, \alpha^*, \beta^*&lt;/script&gt;满足&lt;script type=&quot;math/tex&quot;&gt;p^*=d^*=\mathcal{L}(\omega^*,\alpha^*,\beta^*)&lt;/script&gt;，他们满足下面的KKT条件：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
\frac{\partial}{\partial \omega_i}\mathcal{L}(\omega^*, \alpha^*, \beta^*) &amp;= 0, i=1,\cdots,n \\
\frac{\partial}{\partial \beta_i}\mathcal{L}(\omega^*, \alpha^*, \beta^*) &amp;= 0, i=1,\cdots,l \\
\alpha_i^*g_i(\omega^*) &amp;= 0, i=1,\cdots,k \\
g_i(\omega^*) &amp; \leq 0, i=1,\cdots,k \\
\alpha^* &amp; \geq 0, i=1,\cdots,k
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;相反的，假如存在&lt;script type=&quot;math/tex&quot;&gt;\omega^*, \alpha^*, \beta^*&lt;/script&gt;满足KKT条件，那它也是拉格朗日问题的解。&lt;/p&gt;

&lt;p&gt;在KKT条件的第三式中我们还发现，必须满足&lt;script type=&quot;math/tex&quot;&gt;\alpha_i^*&gt;0&lt;/script&gt;当&lt;script type=&quot;math/tex&quot;&gt;g_i(\omega^*) = 0&lt;/script&gt;。&lt;/p&gt;

&lt;p&gt;（本来以为拉格朗日对偶会很困难，啃下来发现更多的是心理作用，233）&lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;优化间隔分类器&lt;/h2&gt;

&lt;p&gt;回到我们的优化间隔分类器，我们可以把约束写成标准形式：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;g_i(\omega)=-y^{(i)}(\omega^Tx^{(i)}+b)+1 \leq 0&lt;/script&gt;

&lt;p&gt;根据KKT条件，只有在函数间隔是1时才能&lt;script type=&quot;math/tex&quot;&gt;\alpha_i^*&gt;0&lt;/script&gt;。实际上只有很少量的点决定了最大间隔。这些点被称为支持向量。&lt;/p&gt;

&lt;p&gt;把优化问题写成下面的拉格朗日形式，注意我们这里不带&lt;script type=&quot;math/tex&quot;&gt;\beta_i&lt;/script&gt;：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{L}(\omega,b,\alpha)=\frac{1}{2}\|\omega\|^2-\sum_{i=1}^m \alpha_i[y^{(i)}(\omega^Tx^{(i)}+b)-1]&lt;/script&gt;

&lt;p&gt;我们首先找出拉格朗日对偶问题，即先固定&lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;，求对于&lt;script type=&quot;math/tex&quot;&gt;\omega,b&lt;/script&gt;的最小值。分别求&lt;script type=&quot;math/tex&quot;&gt;\mathcal{L}&lt;/script&gt;对&lt;script type=&quot;math/tex&quot;&gt;\omega,b&lt;/script&gt;的偏导：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla_{\omega} \mathcal{L}(\omega,b,\alpha)=\omega - \sum_{i=1}^m\alpha_iy^{(i)}x^{(i)}=0&lt;/script&gt;

&lt;p&gt;由此得到：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\omega=\sum_{i=1}^m \alpha_iy^{(i)}x^{(i)}&lt;/script&gt;

&lt;p&gt;对b求偏导，我们得到：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial}{\partial b} \mathcal{L}(\omega, b, \alpha)=\sum_{i=1}^m \alpha_iy^{(i)}=0&lt;/script&gt;

&lt;p&gt;将&lt;script type=&quot;math/tex&quot;&gt;\omega,b&lt;/script&gt;代回&lt;script type=&quot;math/tex&quot;&gt;\mathcal{L}&lt;/script&gt;表达式，我们得到：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{L}(\omega,b,\alpha)=\sum_{i=1}^m\alpha_i-\frac{1}{2}\sum_{i,j=1}^my^{(i)}y^{(j)}\alpha_i\alpha_j(x^{(i)})^Tx^{(j)}&lt;/script&gt;

&lt;p&gt;最终，我们得到下面的拉格朗日对偶问题：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
\max_{\alpha} \quad &amp; W(\alpha)=\sum_{i=1}^m\alpha_i-\frac{1}{2}\sum_{i,j=1}^my^{(i)}y^{(j)}\alpha_i\alpha_j\langle x^{(i)},x^{(j)} \rangle \\
s.t. \quad &amp; \alpha_i \geq 0, i=1,\cdots,m \\
&amp; \sum_{i=1}^m \alpha_iy^{(i)}=0
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;因为我们满足KKT条件，因此可以通过解答拉格朗日对偶问题来解决原始问题。对偶问题需要用下面的核函数解决。&lt;/p&gt;

&lt;p&gt;在得到&lt;script type=&quot;math/tex&quot;&gt;\alpha^*&lt;/script&gt;后，也就能得到&lt;script type=&quot;math/tex&quot;&gt;\omega^*&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;b^*&lt;/script&gt;可以通过下式得到：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;b^*=-\frac{\max_{i:y^{(i)}=-1} \omega^{*T}x^{(i)}+\min_{i:y^{(i)}=1}\omega^{*T}x^{(i)}}{2}&lt;/script&gt;

&lt;p&gt;进一步讨论一下预测问题，对于新输入的x，我们要计算&lt;script type=&quot;math/tex&quot;&gt;\omega^Tx+b&lt;/script&gt;：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\omega^Tx+b=(\sum_{i=1}^m \alpha_iy^{(i)}x^{(i)})^Tx+b=\sum_{i=1}^m \alpha_iy^{(i)}\langle x^{(i)}, x \rangle + b&lt;/script&gt;

&lt;p&gt;我们注意到预测时也只用到了x的内积，对偶问题中同样只用了内积。此外，只有支持向量的&lt;script type=&quot;math/tex&quot;&gt;\alpha_i&lt;/script&gt;不为0，因此这一步运算花费很少。&lt;/p&gt;

&lt;h2 id=&quot;section-5&quot;&gt;核函数&lt;/h2&gt;
&lt;p&gt;在房价预测中，我们用&lt;script type=&quot;math/tex&quot;&gt;x,x^2,x^3&lt;/script&gt;来得到一个三次方函数。这里我们把原始&lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;称为属性，把&lt;script type=&quot;math/tex&quot;&gt;x,x^2,x^3&lt;/script&gt;称为特征。用&lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt;记为特征映射。在我们的例子里，有：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\phi(x)=\begin{bmatrix}x \\ x^2 \\ x^3\end{bmatrix}&lt;/script&gt;

&lt;p&gt;将内积&lt;script type=&quot;math/tex&quot;&gt;\langle x,z\rangle&lt;/script&gt;用&lt;script type=&quot;math/tex&quot;&gt;\langle \phi(x), \phi(z) \rangle&lt;/script&gt;代替，定义对应的核函数为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;K(x,z)=\phi(x)^T \phi(z)&lt;/script&gt;

&lt;p&gt;巧妙的地方在于可能&lt;script type=&quot;math/tex&quot;&gt;\phi(x)&lt;/script&gt;的计算量很大，但&lt;script type=&quot;math/tex&quot;&gt;K(x,z)&lt;/script&gt;却很容易。实际在算法中我们只需要应用到&lt;script type=&quot;math/tex&quot;&gt;K(x,z)&lt;/script&gt;，却不必知道&lt;script type=&quot;math/tex&quot;&gt;\phi(x)&lt;/script&gt;是多少。举下面两个例子。&lt;/p&gt;

&lt;p&gt;考虑核函数&lt;script type=&quot;math/tex&quot;&gt;K(x,z)=(x^Tz)^2&lt;/script&gt;：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
K(x,z) &amp;= (\sum_{i=1}^n x_iz_i)(\sum_{j=1}^n x_iz_i) \\
&amp;= \sum_{i=1}^n \sum_{j=1}^n x_i x_j z_i z_j \\
&amp;= \sum_{i,j=1}^n (x_ix_j)(z_iz_j)
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;因此特征映射函数为下式，需要&lt;script type=&quot;math/tex&quot;&gt;O(n^2)&lt;/script&gt;时间，而核函数只需要&lt;script type=&quot;math/tex&quot;&gt;O(n)&lt;/script&gt;：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\phi(x)=\begin{bmatrix}x_1x_1 \\ x_1x_2 \\ x_1x_3 \\ x_2x_1 \\ x_2x_2 \\ x_2x_3 \\ x_3x_1 \\ x_3x_2 \\ x_3x_3\end{bmatrix}&lt;/script&gt;

&lt;p&gt;再考虑一个相关的核函数：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;K(x,z)=(x^Tz+c)^2=\sum_{i,j=1}^n (x_ix_j)(z_iz_j)+ \sum_{i=1}^n( \sqrt{2c}x_i)(\sqrt{2c}z_i) + c^2&lt;/script&gt;

&lt;p&gt;此时的特征函数为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\phi(x)=\begin{bmatrix}x_1x_1 \\ x_1x_2 \\ x_1x_3 \\ x_2x_1 \\ x_2x_2 \\ x_2x_3 \\ x_3x_1 \\ x_3x_2 \\ x_3x_3 \\ \sqrt{2c}x_1 \\ \sqrt{2c}x_2 \\ \sqrt{2c}x_3 \\ c\end{bmatrix}&lt;/script&gt;

&lt;p&gt;更一般的，核函数&lt;script type=&quot;math/tex&quot;&gt;K(x,z)=(x^Tz+c)^d&lt;/script&gt;把特征映射到了&lt;script type=&quot;math/tex&quot;&gt;\binom{n+d}{d}&lt;/script&gt;维特征空间。需要&lt;script type=&quot;math/tex&quot;&gt;O(n^d)&lt;/script&gt;时间，而核函数仍只需要&lt;script type=&quot;math/tex&quot;&gt;O(n)&lt;/script&gt;。&lt;/p&gt;

&lt;p&gt;从另一个角度粗略的讲，&lt;script type=&quot;math/tex&quot;&gt;K(x,z)=\phi(x)^T \phi(z)&lt;/script&gt;体现了&lt;script type=&quot;math/tex&quot;&gt;\phi(x)&lt;/script&gt;和&lt;script type=&quot;math/tex&quot;&gt;\phi(z)&lt;/script&gt;之间的接近关系，越接近核函数越大，越远离核函数越小。比如下面的高斯核函数，很好的衡量了x和z之间的关系：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;K(x,z)=\exp(- \frac{\|x-z\|^2}{2\sigma^2})&lt;/script&gt;

&lt;p&gt;但现在有一个问题，我怎么知道这个核函数是有意义的，即能找出特征映射&lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt;满足&lt;script type=&quot;math/tex&quot;&gt;K(x,z)=\phi(x)^T \phi(z)&lt;/script&gt;。&lt;/p&gt;

&lt;p&gt;假设K是一个有效的核函数，给定有限点集&lt;script type=&quot;math/tex&quot;&gt;\{x^{(1)},\cdots,x^{(m)}\}&lt;/script&gt;，令一个核函数矩阵&lt;script type=&quot;math/tex&quot;&gt;K \in \mathbb{R}^{m \times m}, K_{ij}=K(x^{(i)},x^{(j)})&lt;/script&gt;。&lt;/p&gt;

&lt;p&gt;假如K是个有效核函数，则有：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;K_{ij}=K(x^{(i)},x^{(j)})=\phi(x^{(i)})^T\phi(x^{(j)})=\phi(x^{(j)})^T\phi(x^{(i)})=K(x^{(j)},x^{(i)})=K_{ji}&lt;/script&gt;

&lt;p&gt;还能证明核函数矩阵是半正定的：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
z^TKz &amp;= \sum_i\sum_jz_iK_{ij}z_j \\
&amp;=\sum_i\sum_jz_i\phi(x^{(i)})^T\phi(x^{(j)})z_j \\
&amp;=\sum_i\sum_jz_i\sum_k\phi_k(x^{(i)})\phi_k(x^{(j)})z_j\\
&amp;=\sum_k\sum_i\sum_jz_i\phi_k(x^{(i)})\phi_k(x^{(j)})z_j \\
&amp;= \sum_k(\sum_iz_i\phi_k(x^{(i)}))^2 \\
&amp;\geq 0
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;因此当核函数是有效的，核函数矩阵是对称半正定的。事实上这不仅仅是一个必要条件，还是一个充分条件。归纳为Mercer定理。&lt;/p&gt;

&lt;p&gt;除了应用在支持向量机中，核函数还在其他算法中大量应用。这种应用被称为核方法。（个人粗浅的理解，核方法看起来像是小技巧，实际上体现事物的内在本质）。&lt;/p&gt;

&lt;h2 id=&quot;section-6&quot;&gt;规则化和不可分情况&lt;/h2&gt;
&lt;p&gt;之前推导支持向量机时我们假定数据是线性可分割的，通过特征映射我们也能完成非线性分割，但有时候仍然会发生训练结果不好的情况，因为数据中会有噪音或离群点影响。&lt;/p&gt;

&lt;p&gt;为此，我们引入规则化和松弛变量，将优化问题转化为下面的问题：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
\min_{\omega,b, \xi}  \quad  &amp; \frac{1}{2}\|\omega\|^2 + C\sum_{i=1}^m \xi_i\\
\text{s.t.}  \quad &amp; y^{(i)}(\omega^Tx^{(i)}+b)\geq 1-\xi_i, i=1,\cdots,m \\
&amp; \xi_i \geq 0, i=1,\cdots,m
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;构造拉格朗日方程：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{L}(\omega,b,\xi,\alpha,r)=\frac{1}{2}\omega^T\omega+C\sum_{i=1}^m \xi_i - \sum_{i=1}^m \alpha_i[y^{(i)}(x^T\omega+b)-1+\xi_i]-\sum_{i=1}^m r_i\xi_i&lt;/script&gt;

&lt;p&gt;采用对&lt;script type=&quot;math/tex&quot;&gt;\omega,b, \xi&lt;/script&gt;求偏导的方法，可以得到对偶方程：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
\max_{\alpha} \quad &amp; W(\alpha)=\sum_{i=1}^m\alpha_i-\frac{1}{2}\sum_{i,j=1}^my^{(i)}y^{(j)}\alpha_i\alpha_j\langle x^{(i)},x^{(j)} \rangle \\
s.t. \quad &amp; 0 \leq \alpha_i \leq C, i=1,\cdots,m \\
&amp; \sum_{i=1}^m \alpha_iy^{(i)}=0
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;我们注意到对偶方程和未引入松弛变量前几乎一致，除了&lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;取值范围有变化。因为&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial}{\partial \xi_i} \mathcal{L}(\omega,b,\xi,\alpha,r)=C-\alpha_i-r_i=0&lt;/script&gt;

&lt;p&gt;根据KKT条件，还能得出以下结果：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
\alpha_i=0 &amp;\Rightarrow y^{(i)}(\omega^Tx^{(i)}+b) \geq 1 \\
\alpha_i=C &amp;\Rightarrow y^{(i)}(\omega^Tx^{(i)}+b) \leq 1 \\
0 \leq \alpha_i \leq C &amp;\Rightarrow y^{(i)}(\omega^Tx^{(i)}+b) = 1 
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;h2 id=&quot;smo&quot;&gt;SMO算法&lt;/h2&gt;
&lt;p&gt;SMO算法专门用来高效解决支持向量机中推导出来的拉格朗日对偶问题。在讨论SMO算法之前先讲讲坐标上升法。&lt;/p&gt;

&lt;h3 id=&quot;section-7&quot;&gt;坐标上升法&lt;/h3&gt;
&lt;p&gt;坐标上升法是另一种优化方法，类似于前面的梯度下降法和牛顿法。其实质是每次只在一个坐标方向优化，考虑以下的无约束优化问题：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\max_{\alpha} W(\alpha_1, \alpha_2, \cdots, \alpha_m)&lt;/script&gt;

&lt;p&gt;使用以下的算法进行递归直到达到最优解：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; \text{For }i=1,\cdots,m:\\
\alpha_i := \arg \max_{\hat{\alpha_i}} W(\alpha_1,\cdots,\alpha_{i-1},\hat{\alpha_i},\alpha_{i+1},\cdots,\alpha_m)
&lt;/script&gt;

&lt;p&gt;按顺序每一次固定其他，更新一个变量。（更复杂的版本是每一次更新使W增加最快的变量）。&lt;/p&gt;

&lt;p&gt;当函数W是类似于 arg max 这样高效运算的函数时，坐标上升法是一种很有效的算法。&lt;/p&gt;

&lt;h3 id=&quot;smo-1&quot;&gt;SMO&lt;/h3&gt;
&lt;p&gt;我们回到SMO算法来解决拉格朗日对偶问题。
观察约束&lt;script type=&quot;math/tex&quot;&gt;\sum_{i=1}^m \alpha_i y^{(i)}=0&lt;/script&gt;，我们发现&lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;并不完全独立，&lt;script type=&quot;math/tex&quot;&gt;\alpha_1=-y^{(1)}\sum_{i=2}^m\alpha_iy^{(i)}&lt;/script&gt;。&lt;/p&gt;

&lt;p&gt;受坐标上升法的启发，SMO算法的核心就是选择一对参数（使W增长最快的两个）进行更新，直到优化结束。&lt;/p&gt;

&lt;p&gt;假设选择&lt;script type=&quot;math/tex&quot;&gt;\alpha_1,\alpha_2&lt;/script&gt;进行更新：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\alpha_1y^{(1)}+\alpha_2y^{(2)}=-\sum_{i=3}^m \alpha_iy^{(i)}=\zeta&lt;/script&gt;

&lt;p&gt;结合&lt;script type=&quot;math/tex&quot;&gt;0\leq \alpha_i \leq C&lt;/script&gt;的约束，&lt;script type=&quot;math/tex&quot;&gt;\alpha_2&lt;/script&gt;的真实取值范围是&lt;script type=&quot;math/tex&quot;&gt;L \leq \alpha_2 \leq H&lt;/script&gt;。最终更新结果是：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\alpha_2^{new}=
\begin{cases}
H \quad &amp;\text{if } \alpha_2^{new, unclipped} &gt; H \\
\alpha_2^{new, unclipped} \quad &amp;\text{if } L \leq \alpha_2^{new, unclipped} \leq H \\
L 
\end{cases}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\alpha_1^{new}&lt;/script&gt;值可以通过前面的线性关系求得。&lt;/p&gt;

&lt;p&gt;SMO这里讲的还是比较粗略，需要后续再补充。不过支持向量机到此终于完结了，花了三天半时间，也是值得。&lt;/p&gt;
</description>
        <pubDate>Thu, 14 Apr 2016 00:00:00 +0800</pubDate>
        <link>http://anyinlover.github.io/2016/04/14/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/</link>
        <guid isPermaLink="true">http://anyinlover.github.io/2016/04/14/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/</guid>
        
        <category>Ng机器学习系列</category>
        
        <category>机器学习算法</category>
        
        
      </item>
    
      <item>
        <title>生成学习算法</title>
        <description>&lt;p&gt;前面的回归算法和感知机算法都属于判别学习算法，这一章聊聊另一类算法：生成学习算法。这两者的区别可以用以下的比喻，判别学习算法是根据所有猫猫狗狗的特征，建立一个模型，区分出两类动物的分界线。生成学习算法则是分别对猫和狗建立一个模型，然后去对照看跟哪个模型更像。&lt;/p&gt;

&lt;p&gt;生成学习算法的理论依据就是贝叶斯公式，体现了后验概率&lt;script type=&quot;math/tex&quot;&gt;p({y \mid x})&lt;/script&gt;和先验概率&lt;script type=&quot;math/tex&quot;&gt;p(y)&lt;/script&gt;之间的关系：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
p(y \mid x)=\frac{p(x \mid y)p(y)}{p(x)}
&lt;/script&gt;

&lt;p&gt;对于生成学习算法而言，实际分母一致，不需要计算，只需要比较分子大小：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
\arg \max_y p(y \mid x) &amp;= \arg \max_y \frac{p(x \mid y)p(y)}{p(x)}\\
&amp;=\arg \max_y p(x \mid y)p(y)
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;h2 id=&quot;section&quot;&gt;高斯判别分析&lt;/h2&gt;
&lt;p&gt;当假设p(x \mid y)分布遵循多元正态分布时，我们可以使用高斯判别分析算法。在这之前先简要介绍多元正态分布。&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;多元正态分布&lt;/h3&gt;
&lt;p&gt;当正态分布从一元拓展到多元时，正态分布概率密度函数也需要做出相应的改变：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
p(x;\mu,\Sigma)=\frac{1}{(2\pi)^{n/2} \mid  \Sigma  \mid ^{1/2}} \exp\left(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right)
&lt;/script&gt;

&lt;p&gt;其中，&lt;script type=&quot;math/tex&quot;&gt;\mu \in \mathbb{R}^n&lt;/script&gt; 是一个平均数向量，&lt;script type=&quot;math/tex&quot;&gt;\Sigma \in \mathbb{R}^{n\times n}&lt;/script&gt;是一个协方差矩阵。粗略一看，多元正态分布的表达式和一元正态分布还是有几分神似的。&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;高斯判别分布模型&lt;/h3&gt;
&lt;p&gt;对于二元分类问题，满足以下假设时可以使用高斯判别分布模型：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
y &amp;\sim Bernoulli(\phi) \\
x \mid y=0 &amp;\sim \mathcal{N}(\mu_0,\Sigma)\\
x \mid y=1 &amp;\sim \mathcal{N}(\mu_1,\Sigma)
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;即下列分布函数：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
p(y) &amp;= \phi^y(1-\phi)^{1-y} \\
p(x \mid y=0) &amp;= \frac{1}{(2\pi)^{n/2} \mid  \Sigma  \mid ^{1/2}} \exp\left(-\frac{1}{2}(x-\mu_0)^T\Sigma^{-1}(x-\mu_0)\right)\\
p(x \mid y=1) &amp;= \frac{1}{(2\pi)^{n/2} \mid  \Sigma  \mid ^{1/2}} \exp\left(-\frac{1}{2}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1)\right)
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;注意两个多元正态分布的平均数向量不同，协方差矩阵是一致的。然后我们就可以求解最大似然函数了：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
\ell(\phi,\mu_0,\mu_1,\Sigma) &amp;= \log \prod_{i=1}^m p(x^{(i)},y^{(i)};\phi,\mu_0,\mu_1,\Sigma)\\
&amp;= \log \prod_{i=1}^m p(x^{(i)} \mid y^{(i)};\mu_0,\mu_1,\Sigma)p(y^{(i)};\phi)
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;各参数可解得：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
\phi &amp;= \frac{1}{m}\sum_{i=1}^m1\{y^{(1)}\} \\
\mu_0 &amp;= \frac{\sum_{i=1}^m1\{y^{i}=0\}x^{(i)}}{\sum_{i=1}^m1\{y^{i}=0\}}\\
\mu_1 &amp;= \frac{\sum_{i=1}^m1\{y^{i}=1\}x^{(i)}}{\sum_{i=1}^m1\{y^{i}=1\}}\\
\Sigma &amp;= \frac{1}{m}\sum_{i=1}^m(x^{(i)}-\mu_{y^{(i)}})(x^{(i)}-\mu_{y^{(i)}})^T
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;h3 id=&quot;section-3&quot;&gt;高斯判别分布和逻辑回归&lt;/h3&gt;
&lt;p&gt;从高斯判别分布可以推出逻辑回归（表问我怎么推~）：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(y=1 \mid x;\phi,\Sigma,\mu_0,\mu_1)=\frac{1}{1+\exp(-\theta^Tx)}&lt;/script&gt;

&lt;p&gt;实际上高斯判别分布应用了更强的假设，即&lt;script type=&quot;math/tex&quot;&gt;p(x \mid y)&lt;/script&gt;是一个多元高斯分布。因此可以从高斯判别分布可以推导到逻辑回归，但满足逻辑回归的不一定满足高斯判别分布，比如泊松判别分布也能推导到高斯分布。&lt;/p&gt;

&lt;p&gt;这也造成了高斯判别分布的性质，当原始数据吻合高斯分布时，这是一种很有效很精确的算法。但更多时候数据不能很好的满足高斯分布，这时候高斯判别分布就失效了，相比而言，逻辑回归更稳定，也更常用。&lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;朴素贝叶斯&lt;/h2&gt;
&lt;p&gt;朴素贝叶斯是个大名鼎鼎的算法，不同于高斯判别分布应用于连续型输入，朴素贝叶斯应用于离散型输入，其最常用于文本分类中，比如判别是否为垃圾邮件。&lt;/p&gt;

&lt;p&gt;在文本分类中，一张词汇表作为一个特征向量，文本中含这个词则为1，不含则为0，最后的结果示例如下：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x=\begin{bmatrix}1\\0\\0\\\vdots\\1\\\vdots\\0\end{bmatrix}&lt;/script&gt;

&lt;p&gt;要实现朴素贝叶斯算法，需要朴素贝叶斯假设成立。即特征值取值概率是完全相互独立的：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_1,\cdots,x_n \mid y)=\prod_{i=1}^n p(x_i \mid y)&lt;/script&gt;

&lt;p&gt;即使建立在这种强假设上，朴素贝叶斯经常很好使。&lt;/p&gt;

&lt;p&gt;根据贝叶斯法则，需要确定的模型参数有&lt;script type=&quot;math/tex&quot;&gt;\phi_{i \mid y=1}=p(x_i=1 \mid y=1),\phi_{i \mid y=0}=p(x_i=1 \mid y=0),\phi_y=p(y=1)&lt;/script&gt;。计算最大似然函数：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{L}(\phi_y,\phi_{j \mid y=0},\phi_{j \mid y=1})=\prod_{i=1}^m p(x^{(i)},y^{(i)})
&lt;/script&gt;

&lt;p&gt;求最大值可以解出各参数值。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
\phi_{j \mid y=1}&amp;=\frac{\sum_{i=1}^m1\{x_j^{(i)}=1\wedge y^{(i)}=1\}}{\sum_{i=1}^m1\{y^{(i)}=1\}}\\
\phi_{j \mid y=0}&amp;=\frac{\sum_{i=1}^m1\{x_j^{(i)}=1\wedge y^{(i)}=0\}}{\sum_{i=1}^m1\{y^{(i)}=0\}}\\
\phi_y&amp;= \frac{\sum_{i=1}^m1\{y^{(i)}=1\}}{m}
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\wedge&lt;/script&gt;表示与运算，两边都满足才为真。这三个表达式其实都很直观，就是计数。总数里为1占得比例。&lt;/p&gt;

&lt;p&gt;预测函数也可以给出来：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
p(y=1 \mid x)&amp;=\frac{p(x \mid y=1)p(y=1)}{p(x)}\\
&amp;=\frac{(\prod_{i=1}^np(x_i \mid y=1))p(y=1)}{(\prod_{i=1}^np(x_i \mid y=1))p(y=1)+(\prod_{i=1}^np(x_i \mid y=0))p(y=0)}
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;对于连续型输入，也可以通过分段处理来离散化，然后使用朴素贝叶斯算法。&lt;/p&gt;

&lt;p&gt;朴素贝叶斯算法这一块缺少实践感悟，后续需要再来研究。&lt;/p&gt;

&lt;h3 id=&quot;section-5&quot;&gt;拉普拉斯平滑处理&lt;/h3&gt;
&lt;p&gt;朴素贝叶斯算法存在一个问题，对于稀疏数据敏感。比如文本分类时有从未出现过的词，则&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\phi_{j \mid y=1}=\phi_{j \mid y=0}=0
&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(y=1 \mid x)=\frac{0}{0}&lt;/script&gt;

&lt;p&gt;这样计算就会出问题，换一个角度，从未出现过的词不代表以后也不会出现。因此简单把其概率置为0是不合理的。&lt;/p&gt;

&lt;p&gt;对于多元分类问题，假设z取值{1,…,k}，进行m次独立观察，则根据朴素贝叶斯算法：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\phi_j=\frac{\sum_{i=1}^m 1\{z^{(i)}=j\}}{m}
&lt;/script&gt;

&lt;p&gt;利用拉普拉斯平滑处理，可以解决这个问题，保证了每种情况至少有一个大于0的概率。同时保证&lt;script type=&quot;math/tex&quot;&gt;\sum_{j=1}^k \phi_j=1&lt;/script&gt;。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\phi_j=\frac{\sum_{i=1}^k 1\{z^{(i)}=j\}+1}{m+k}
&lt;/script&gt;

&lt;p&gt;回头看上一节的垃圾邮件朴素贝叶斯，应用拉普拉斯平滑处理后：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
\phi_{j \mid y=1}&amp;=\frac{\sum_{i=1}^m1\{x_j^{(i)}=1\wedge y^{(i)}=1\}+1}{\sum_{i=1}^m1\{y^{(i)}=1\}+2}\\
\phi_{j \mid y=0}&amp;=\frac{\sum_{i=1}^m1\{x_j^{(i)}=1\wedge y^{(i)}=0\}+1}{\sum_{i=1}^m1\{y^{(i)}=0\}+2}
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;注意在垃圾邮件的实践应用中，&lt;script type=&quot;math/tex&quot;&gt;\phi_y&lt;/script&gt;可以不用拉普拉斯平滑处理，因为一般正常邮件和垃圾邮件会有一个比较合理的比例。不可能出现为0的情况。&lt;/p&gt;

&lt;h3 id=&quot;section-6&quot;&gt;文本分类事件模型&lt;/h3&gt;
&lt;p&gt;前面使用的朴素贝叶斯模型被称为多元伯努利事件模型，在文本分类中，还有另一种针对邮件而不是词汇表处理的朴素贝叶斯模型，称为多项式事件模型。&lt;/p&gt;

&lt;p&gt;我们让&lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt;表示邮件中第i个词，在{1,…, \mid V \mid }中取值， \mid V \mid 代表词汇表大小。每一个词取值可能性相同，即满足多项式分布。&lt;/p&gt;

&lt;p&gt;模型的参数有&lt;script type=&quot;math/tex&quot;&gt;\phi_y=p(y),\phi_{k \mid y=1}=p(x_j=k \mid y=1),\phi_{k \mid y=0}=p(x_j=k \mid y=0)&lt;/script&gt;，对于训练集&lt;script type=&quot;math/tex&quot;&gt;\{(x^{(i)},y^{(i)});i=1,...,m\}, x^{(i)}=(x_1^{(i)},x_2^{(i)},\cdots,x_{n_i}^{(i)}),n_i&lt;/script&gt;代表第i个训练集中单词总量。得出最大似然函数：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
\mathcal{L}(\phi_y,\phi_{k \mid y=0},\phi_{k \mid y=1})&amp;=\prod_{i=1}^m p(x^{(i)},y^{(i)})\\
&amp;=\prod_{i=1}^m\left(\prod_{j=1}^mp(x_j^{(i)} \mid y;\phi_{k \mid y=0},\phi_{k \mid y=1})\right)p(y^{(i)};\phi_y)
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;求最大值求解得到：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
\phi_{k \mid y=1}&amp;=\frac{\sum_{i=1}^m\sum_{j=1}^{n_i}1\{x_j^{(i)}=k\wedge y^{(i)}=1\}}{\sum_{i=1}^m1\{y^{(i)}=1\}n_i}\\
\phi_{k \mid y=0}&amp;=\frac{\sum_{i=1}^m\sum_{j=1}^{n_i}1\{x_j^{(i)}=k\wedge y^{(i)}=0\}}{\sum_{i=1}^m1\{y^{(i)}=0\}n_i}\\
\phi_y&amp;= \frac{\sum_{i=1}^m1\{y^{(i)}=1\}}{m}
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;对其使用拉普拉斯平滑：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
\phi_{k \mid y=1}&amp;=\frac{\sum_{i=1}^m\sum_{j=1}^{n_i}1\{x_j^{(i)}=k\wedge y^{(i)}=1\}+1}{\sum_{i=1}^m1\{y^{(i)}=1\}n_i+ \mid V \mid }\\
\phi_{k \mid y=0}&amp;=\frac{\sum_{i=1}^m\sum_{j=1}^{n_i}1\{x_j^{(i)}=k\wedge y^{(i)}=0\}+1}{\sum_{i=1}^m1\{y^{(i)}=0\}n_i+ \mid V \mid }
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;朴素贝叶斯不是最好的分类方法，但常常很有效。由于其简洁简单，朴素贝叶斯经常值得一试。 \mid &lt;/p&gt;
</description>
        <pubDate>Tue, 12 Apr 2016 00:00:00 +0800</pubDate>
        <link>http://anyinlover.github.io/2016/04/12/%E7%94%9F%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/</link>
        <guid isPermaLink="true">http://anyinlover.github.io/2016/04/12/%E7%94%9F%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/</guid>
        
        <category>Ng机器学习系列</category>
        
        <category>机器学习算法</category>
        
        
      </item>
    
      <item>
        <title>随机分布</title>
        <description>&lt;p&gt;随机分布属于概率统计的内容，同样在机器学习中经常遇到，线性回归被视为高斯分布，逻辑回归则是伯努利分布，因此在这里专列一篇介绍七个常见的随机分布，分成离散随机分布和连续随机分布两大块。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;离散随机分布&lt;/h2&gt;
&lt;p&gt;介绍四种离散随机分布，分别是伯努利分布，二项分布，几何分布和泊松分布。离散随机分布用概率表达。&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;伯努利分布&lt;/h3&gt;
&lt;p&gt;伯努利分布是最简单的一种分布，抛一枚头向上概率为p的硬币，最终结果的分布。&lt;script type=&quot;math/tex&quot;&gt;X\sim Bernoulli(p),0\leq p\leq1&lt;/script&gt;：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

p(x)=
\begin{cases}
p &amp; \quad \text{if }x=1\\
1-p &amp; \quad \text{if }x=0
\end{cases}
 %]]&gt;&lt;/script&gt;

&lt;h3 id=&quot;section-2&quot;&gt;二项分布&lt;/h3&gt;
&lt;p&gt;独立抛n次头向上概率为p的硬币，最终有x次头向上的概率。&lt;script type=&quot;math/tex&quot;&gt;X\sim Binomial(n,p), 0\leq \ p\leq 1&lt;/script&gt;：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
p(x)=\binom{n}{x}p^x(1-p)^{n-x}
&lt;/script&gt;

&lt;h3 id=&quot;section-3&quot;&gt;几何分布&lt;/h3&gt;
&lt;p&gt;连续抛头向上概率为p的硬币，直到第x次头向上的概率。&lt;script type=&quot;math/tex&quot;&gt;X\sim Geometric(p), 0\leq \ p\leq 1&lt;/script&gt;：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
p(x)=p(1-p)^{x-1}
&lt;/script&gt;

&lt;h3 id=&quot;section-4&quot;&gt;泊松分布&lt;/h3&gt;
&lt;p&gt;终于到了泊松分布，说实话这个分布一直听到，却一直没有了解其本质。这里先给出泊松分布的公式，再尝试做一个推导，加深一下对泊松分布的理解。&lt;script type=&quot;math/tex&quot;&gt;X\sim Poisson(\lambda), \lambda &gt; 0&lt;/script&gt;：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
p(x)=e^{-\lambda}\frac{\lambda^x}{x!}
&lt;/script&gt;

&lt;p&gt;泊松分布可以从二项分布推演出来。在二项分布中，期望&lt;script type=&quot;math/tex&quot;&gt;\lambda=np&lt;/script&gt;，在p固定的情况下，&lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;会随着n的增大而增大。现在考虑期望固定，n无限大的情况，二项分布的公式就会发生很大的变化：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
p_{\lambda}(x)&amp;=\lim_{n \to \infty}\frac{n!}{x!(n-x)!}p^x(1-p)^{n-x} \\
&amp;=\lim_{n \to \infty}\frac{n!}{x!(n-x)!}(\frac{\lambda}{n})^x(1-\frac{\lambda}{n})^{n-x}\\
&amp;=\frac{\lambda^x}{x!}\lim_{n \to \infty}\frac{n(n-1)\cdots(n-x+1)}{n^x}(1-\frac{\lambda}{n})^x\lim_{n \to \infty}(1-\frac{\lambda}{n})^n\\
&amp;=\frac{\lambda^x}{x!}\cdot1\cdot e^{-\lambda}\\
&amp;=e^{-\lambda}\frac{\lambda^x}{x!}
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;泊松分布必须满足下面三个性质，和上面的假设和推导相互印证。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;任意单位时间长度内，到达率稳定。即&lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;固定。&lt;/li&gt;
  &lt;li&gt;未来与过去无关。即n永远为无穷次。&lt;/li&gt;
  &lt;li&gt;在极小的时间内，1次发生的概率很小，0次发生的概率很大。即p很小。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;section-5&quot;&gt;连续随机分布&lt;/h2&gt;
&lt;p&gt;介绍三种连续随机分布，分别是均匀分布，指数分布和正态分布。注意连续分布是用概率密度表示的。&lt;/p&gt;

&lt;h3 id=&quot;section-6&quot;&gt;均匀分布&lt;/h3&gt;
&lt;p&gt;均匀分布也好理解，在取值区间内概率相等。&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
X\sim Uniform(a,b), a&lt;b %]]&gt;&lt;/script&gt;：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

f(x)=
\begin{cases}
\frac{1}{b-a} &amp; \quad \text{if }a\leq x \leq b \\
0 &amp; \quad \text{otherwise}
\end{cases}
 %]]&gt;&lt;/script&gt;

&lt;h3 id=&quot;section-7&quot;&gt;指数分布&lt;/h3&gt;
&lt;p&gt;指数分布是另一个神奇的分布，它和泊松分布是一对好基友。先给出表达式，后面再给解释。&lt;script type=&quot;math/tex&quot;&gt;X\sim Exponential(\lambda), \lambda&gt;0&lt;/script&gt;：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

f(x)=
\begin{cases}
\lambda e^{-\lambda x} &amp; \quad \text{if }x\geq 0 \\
0 &amp; \quad \text{otherwise}
\end{cases}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;当泊松分布用来衡量事件随时间分布时，指数分布可以用来描述事件间时间段长度。令&lt;script type=&quot;math/tex&quot;&gt;\lambda=\lambda&#39;t&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;\lambda&#39;&lt;/script&gt;表示单位时间的平均到达率。此时&lt;script type=&quot;math/tex&quot;&gt;p(x)=e^{-\lambda&#39;t}(\lambda&#39;t)^x/x!&lt;/script&gt;，在t时间内一次都没发生的概率是&lt;script type=&quot;math/tex&quot;&gt;e^{-\lambda&#39; t}&lt;/script&gt;，因此事件发生的概率是：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
p(T\leq t)=1-e^{-\lambda&#39; t}
&lt;/script&gt;

&lt;p&gt;事件间时长描述可求导得到：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
f(t)=p&#39;(T\leq t)=\lambda&#39; e^{-\lambda&#39; t}
&lt;/script&gt;

&lt;p&gt;这就是我们的指数分布！同时我们也注意到两个分布原表达式中的&lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;是不一致的。也难怪，前者是用来表达概率的，后者是用来表达概率密度的。&lt;/p&gt;

&lt;h3 id=&quot;section-8&quot;&gt;正态分布&lt;/h3&gt;
&lt;p&gt;正态分布是老相识了，也就是高斯分布。正态分布也有很多神奇的性质，限于时间，下次有机会再写。&lt;script type=&quot;math/tex&quot;&gt;X \sim Normal(\mu,\sigma^2)&lt;/script&gt;：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2\sigma^2}(x-\mu)^2}
&lt;/script&gt;

&lt;p&gt;七种常见的随机分布暂时写到这里，其实还有很多可以补充。比如图表，均值和方差。这点内容花了我两个半小时，水还是深啊。&lt;/p&gt;

</description>
        <pubDate>Mon, 11 Apr 2016 00:00:00 +0800</pubDate>
        <link>http://anyinlover.github.io/2016/04/11/%E9%9A%8F%E6%9C%BA%E5%88%86%E5%B8%83/</link>
        <guid isPermaLink="true">http://anyinlover.github.io/2016/04/11/%E9%9A%8F%E6%9C%BA%E5%88%86%E5%B8%83/</guid>
        
        <category>Ng机器学习系列</category>
        
        <category>数学基础</category>
        
        
      </item>
    
      <item>
        <title>广义线性模型</title>
        <description>&lt;p&gt;这一章讲讲广义线性模型。我刚看到这一章的时候，觉得很神奇。知识的境界不就是归一么。能把具体的模型一般化了，这本身就是件美丽的事。这里就是把线性回归和逻辑回归都归入了广义线性模型（GLMs)。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;指数分布族&lt;/h2&gt;
&lt;p&gt;在讲广义线性模型前，需要先了解指数分布族。凡是能表达成下面的形式的，都属于指数分布族。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(y;\eta)=b(y)\exp(\eta^TT(y)-a(\eta))&lt;/script&gt;

&lt;p&gt;其中&lt;script type=&quot;math/tex&quot;&gt;\eta&lt;/script&gt;被称为自然参数，&lt;script type=&quot;math/tex&quot;&gt;T(y)&lt;/script&gt;是充分统计量，对机器学习来说一般&lt;script type=&quot;math/tex&quot;&gt;T(y)=y&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;a(\eta)&lt;/script&gt;是对数部分函数，其作用是做一个正态化常量。&lt;/p&gt;

&lt;p&gt;下面可以证明伯努利分布和高斯分布都属于指数分布族。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
p(y;\phi)&amp;=\phi^y(1-\phi)^{1-y}\\
&amp;=\exp(y\log\phi+(1-y)\log(1-\phi))\\
&amp;=\exp((\log(\frac{\phi}{1-\phi}))y+\log(1-\phi))
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;因此有&lt;script type=&quot;math/tex&quot;&gt;\eta=\log(\frac{\phi}{1-\phi})&lt;/script&gt;，得到&lt;script type=&quot;math/tex&quot;&gt;\phi=1/(1+e^{-\eta})&lt;/script&gt;。
其他几个参数也水到渠成：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
T(y) &amp;= y \\
a(\eta) &amp;= -\log(1-\phi)\\
&amp;= \log(1+e^\eta)\\
b(y) &amp;= 1
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;对于高斯分布而言，因为&lt;script type=&quot;math/tex&quot;&gt;\sigma^2&lt;/script&gt;对于最终的结果没有影响，因此取&lt;script type=&quot;math/tex&quot;&gt;\sigma^2=1&lt;/script&gt;。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
p(y;\mu)&amp;=\frac{1}{\sqrt{2\pi}}\exp(-\frac{1}{2}(y-\mu)^2)\\
&amp;=\frac{1}{\sqrt{2\pi}}\exp(-\frac{1}{2}y^2)\cdot\exp(\mu y-\frac{1}{2}\mu^2)
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;因此有：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
\eta &amp;= \mu \\
T(y) &amp;= y \\
a(\eta) &amp;= \mu^2/2=\eta^2/2\\
b(y) &amp;=(1/\sqrt(2\pi))\exp(-y^2/2)
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;此外还有多种分布也是属于指数分布族：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;多项式分布：多个离散输出建模&lt;/li&gt;
  &lt;li&gt;泊松分布：对计数过程建模&lt;/li&gt;
  &lt;li&gt;伽马分布和指数分布：对连续非负随机变量建模，如时间间隔&lt;/li&gt;
  &lt;li&gt;贝塔分布和狄利克雷分布：对概率分布建模&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这么一看上次总结的七个分布完全不够用啊，逃~&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;构建广义线性模型&lt;/h2&gt;
&lt;p&gt;针对分类问题或回归问题，需要构造一个关于x的函数来预测y的值。满足三个假设可以构建广义线性模型：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;y\mid x;\theta \sim ExponentialFamily(\eta)&lt;/script&gt; 即y的分布要满足某些指数分布族。&lt;/li&gt;
  &lt;li&gt;给定x，目标是预测&lt;script type=&quot;math/tex&quot;&gt;T(y)&lt;/script&gt;的期望值，大多数情况下即y的期望值。&lt;script type=&quot;math/tex&quot;&gt;h(x)=E[y \mid  x]&lt;/script&gt;。&lt;/li&gt;
  &lt;li&gt;自然参数&lt;script type=&quot;math/tex&quot;&gt;\eta&lt;/script&gt;和输入x成线性关系：&lt;script type=&quot;math/tex&quot;&gt;\eta=\theta^Tx&lt;/script&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这三个假说很容易理解，下面对线性回归和逻辑回归的推导也应用了三个假说，同时也证明了我们的构造的正确性。&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;线性回归&lt;/h3&gt;
&lt;p&gt;对于线性回归而言，其满足高斯分布，预测函数构造如下：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
h_{\theta}(x)&amp;=E[y \mid x;\theta]\\
&amp;=\mu\\
&amp;=\eta\\
&amp;=\theta^Tx
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;h3 id=&quot;section-3&quot;&gt;逻辑回归&lt;/h3&gt;
&lt;p&gt;对于逻辑回归而言，其满足伯努利分布，预测函数构造如下：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
h_{\theta}(x)&amp;=E[y \mid x;\theta]\\
&amp;=\theta\\
&amp;=1/(1+e^{-\eta})\\
&amp;=1/(1+e^{-\theta^Tx})
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;所以，前面逻辑回归时用逻辑斯蒂函数不是没有道理的。&lt;/p&gt;

&lt;h3 id=&quot;softmax-&quot;&gt;Softmax 回归&lt;/h3&gt;
&lt;p&gt;Softmax回归是逻辑回归的一般化，当输出有k个离散值时，y呈多项式分布，可以用Softmax回归刻画。&lt;/p&gt;

&lt;p&gt;y可以取k个值，每一个取值的概率为&lt;script type=&quot;math/tex&quot;&gt;\theta_1,\cdots,\theta_k&lt;/script&gt;，但实际上这k个概率不是相互独立的，&lt;script type=&quot;math/tex&quot;&gt;\theta_k=1-\sum_{i=1}^{k-1}\theta_i&lt;/script&gt;。&lt;/p&gt;

&lt;p&gt;为把多项式分布表示成指数分布族，定义&lt;script type=&quot;math/tex&quot;&gt;T(y)\in \mathbb{R}^{k-1}&lt;/script&gt;，而不再是y了：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;T(1)=\begin{bmatrix}1\\0\\0\\\vdots\\0\end{bmatrix}, T(2)=\begin{bmatrix}0\\1\\0\\\vdots\\0\end{bmatrix}, T(3)=\begin{bmatrix}0\\0\\1\\\vdots\\0\end{bmatrix}, \cdots, T(k-1)=\begin{bmatrix}0\\0\\0\\\vdots\\1\end{bmatrix},T(k)=\begin{bmatrix}0\\0\\0\\\vdots\\0\end{bmatrix}&lt;/script&gt;

&lt;p&gt;用一种新的表达式来表示上式，&lt;script type=&quot;math/tex&quot;&gt;(T(y))_i=1\{y=i\}&lt;/script&gt;。我们可以开始证明Softmax分布也是指数分布族的一种了：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
p(y;\phi)&amp;=\phi_1^{1\{y=1\}}\phi_2^{1\{y=2\}}\cdots\phi_k^{1\{y=k\}}\\
&amp;=\phi_1^{(T(y))_1}\phi_2^{(T(y))_2}\cdots\phi_k^{1-\sum_{i=1}^{k-1}(T(y))_i}\\
&amp;=\exp((T(y))_1\log(\phi_1)+(T(y))_2\log(\phi_2)+\cdots+(1-\sum_{i=1}^{k-1}(T(y))_i)\log(\phi_k))\\
&amp;=\exp((T(y))_1\log(\phi_1/\phi_k)+(T(y))_2\log(\phi_2/\phi_k)+\cdots+(T(y))_{k-1}\log(\phi_{k-1}/\phi_k)+\log(\phi_k))\\
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;可以得出指数分布族的各参数：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
\eta &amp;= \begin{bmatrix}\log(\phi_1/\phi_k)\\\log(\phi_2/\phi_k)\\\vdots\\\log(\phi_{k-1}/\phi_k)\end{bmatrix}\\
a(\eta)&amp;=-\log(\eta_k)\\
b(y)&amp;=1
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;下面需要推导从&lt;script type=&quot;math/tex&quot;&gt;\eta&lt;/script&gt;到&lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt;的映射。
由上面可知，&lt;script type=&quot;math/tex&quot;&gt;\eta_i=\log(\phi_i/\phi_k)&lt;/script&gt;，同时定义&lt;script type=&quot;math/tex&quot;&gt;\eta_k=\log(\phi_k/\phi_k)=0&lt;/script&gt;。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
e^{\eta_i}&amp;=\frac{\phi_i}{\phi_k}\\
\phi_ke^{\eta_i}&amp;=\phi_k\\
\phi_k\sum_{i=1}^ke^{\eta_i}&amp;=\sum_{i=1}^k\phi_i=1\\
\phi_i&amp;=\frac{e^{\eta_i}}{\sum_{j=1}^k e^{\eta_j}}
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;上面的表达式可以求解&lt;script type=&quot;math/tex&quot;&gt;\phi_1,\cdots,\phi_{k-1}&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;\phi_k=1/\sum_{i=1}^ke^{\eta_i}&lt;/script&gt;，这个从&lt;script type=&quot;math/tex&quot;&gt;\eta&lt;/script&gt;到&lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt;的映射函数称为softmax函数。&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\eta&lt;/script&gt;和x还是线性关系，&lt;script type=&quot;math/tex&quot;&gt;\eta_i=\theta_i^Tx,(i=1,\cdots,k-1)&lt;/script&gt;，其中&lt;script type=&quot;math/tex&quot;&gt;\theta_1,\cdots,\theta_{k-1}\in \mathbb{R}^{n+1}&lt;/script&gt;是我们模型的参数，定义&lt;script type=&quot;math/tex&quot;&gt;\theta_k=0&lt;/script&gt;，则&lt;script type=&quot;math/tex&quot;&gt;\eta_k=\theta_k^Tx=0&lt;/script&gt;，因此分布概率可表达成下式：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
p(y=i \mid x;\theta) &amp;= \phi_i\\
&amp;=\frac{e^{\eta_i}}{\sum_{j=1}^ke^{\eta_j}}\\
&amp;=\frac{e^{\theta_i^Tx}}{\sum_{j=1}^ke^{\theta_j^Tx}}
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;我们的预测函数则可以表达为下式：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
h_{\theta}(x) &amp;= E[T(y) \mid x;\theta]\\
&amp;= \begin{bmatrix}\phi_1\\\phi_2\\\vdots\\\phi_{k-1}\end{bmatrix}\\
&amp;=\begin{bmatrix}\frac{e^{\theta_1^Tx}}{\sum_{j=1}^ke^{\theta_j^Tx}}\\
\frac{e^{\theta_2^Tx}}{\sum_{j=1}^ke^{\theta_j^Tx}}\\
\vdots\\
\frac{e^{\theta_{k-1}^Tx}}{\sum_{j=1}^ke^{\theta_j^Tx}}\end{bmatrix}
\end{align}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;最后讨论一下参数拟合。还是采用和逻辑回归同样的方法，求最大似然函数的最大值，可以使用梯度下降法或牛顿法：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{align}
\ell(\theta)&amp;=\sum_{i=1}^m\log p(y^{(i)} \mid x^{(i)};\theta)\\
&amp;=\sum_{i=1}^m\log \prod_{i=1}^k(\frac{e^{\theta_l^Tx}}{\sum_{j=1}^ke^{\theta_j^Tx}})^{1\{y^{(i)}=l\}}
\end{align}
 %]]&gt;&lt;/script&gt;
</description>
        <pubDate>Mon, 11 Apr 2016 00:00:00 +0800</pubDate>
        <link>http://anyinlover.github.io/2016/04/11/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/</link>
        <guid isPermaLink="true">http://anyinlover.github.io/2016/04/11/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/</guid>
        
        <category>Ng机器学习系列</category>
        
        <category>机器学习算法</category>
        
        
      </item>
    
  </channel>
</rss>
